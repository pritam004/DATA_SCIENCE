{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install OpenAI Gym 4 . Implement a neural network based policy-gradient solution for the\n",
    "CartPole-v1 and MountainCarContinuous-v0 environments from OpenAI Gym. Plot episode rewards as a\n",
    "function of the number of training episodes and save it as “2.png”. As in the case of Problem 1, you are free\n",
    "to choose the architecture of the policy neural networks. Specify all the hyper-parameters used by you along\n",
    "with the training strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''All codes and examples are reproduced as given in the book \n",
    "Deep Reinforcement Learning Hands-On by Maxim Lapan'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the beginning, we define hyperparameters \n",
    "\n",
    "Entropy beta value is the scale of the entropy bonus. The REWARD_STEPS value specifies how\n",
    "many steps ahead the Bellman equation is unrolled to estimate the discounted\n",
    "total reward of every transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "ENTROPY_BETA = 0.01\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "REWARD_STEPS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that despite the fact our\n",
    "network returns probabilities, we’re not applying softmax nonlinearity to the\n",
    "output. The reason behind this is that we’ll use the PyTorch log_softmax\n",
    "function to calculate the logarithm of the softmax output at once. This way of\n",
    "calculation is much more numerically stable, but we need to remember that\n",
    "output from the network is not probability, but raw scores (usually called logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(PGN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "29: reward:  29.00, mean_100:  29.00, episodes: 1\n",
      "52: reward:  23.00, mean_100:  26.00, episodes: 2\n",
      "63: reward:  11.00, mean_100:  21.00, episodes: 3\n",
      "80: reward:  17.00, mean_100:  20.00, episodes: 4\n",
      "92: reward:  12.00, mean_100:  18.40, episodes: 5\n",
      "121: reward:  29.00, mean_100:  20.17, episodes: 6\n",
      "135: reward:  14.00, mean_100:  19.29, episodes: 7\n",
      "150: reward:  15.00, mean_100:  18.75, episodes: 8\n",
      "180: reward:  30.00, mean_100:  20.00, episodes: 9\n",
      "195: reward:  15.00, mean_100:  19.50, episodes: 10\n",
      "227: reward:  32.00, mean_100:  20.64, episodes: 11\n",
      "241: reward:  14.00, mean_100:  20.08, episodes: 12\n",
      "260: reward:  19.00, mean_100:  20.00, episodes: 13\n",
      "280: reward:  20.00, mean_100:  20.00, episodes: 14\n",
      "295: reward:  15.00, mean_100:  19.67, episodes: 15\n",
      "319: reward:  24.00, mean_100:  19.94, episodes: 16\n",
      "361: reward:  42.00, mean_100:  21.24, episodes: 17\n",
      "385: reward:  24.00, mean_100:  21.39, episodes: 18\n",
      "398: reward:  13.00, mean_100:  20.95, episodes: 19\n",
      "415: reward:  17.00, mean_100:  20.75, episodes: 20\n",
      "424: reward:   9.00, mean_100:  20.19, episodes: 21\n",
      "461: reward:  37.00, mean_100:  20.95, episodes: 22\n",
      "478: reward:  17.00, mean_100:  20.78, episodes: 23\n",
      "508: reward:  30.00, mean_100:  21.17, episodes: 24\n",
      "557: reward:  49.00, mean_100:  22.28, episodes: 25\n",
      "578: reward:  21.00, mean_100:  22.23, episodes: 26\n",
      "595: reward:  17.00, mean_100:  22.04, episodes: 27\n",
      "633: reward:  38.00, mean_100:  22.61, episodes: 28\n",
      "649: reward:  16.00, mean_100:  22.38, episodes: 29\n",
      "664: reward:  15.00, mean_100:  22.13, episodes: 30\n",
      "688: reward:  24.00, mean_100:  22.19, episodes: 31\n",
      "706: reward:  18.00, mean_100:  22.06, episodes: 32\n",
      "715: reward:   9.00, mean_100:  21.67, episodes: 33\n",
      "748: reward:  33.00, mean_100:  22.00, episodes: 34\n",
      "764: reward:  16.00, mean_100:  21.83, episodes: 35\n",
      "790: reward:  26.00, mean_100:  21.94, episodes: 36\n",
      "840: reward:  50.00, mean_100:  22.70, episodes: 37\n",
      "880: reward:  40.00, mean_100:  23.16, episodes: 38\n",
      "896: reward:  16.00, mean_100:  22.97, episodes: 39\n",
      "907: reward:  11.00, mean_100:  22.68, episodes: 40\n",
      "927: reward:  20.00, mean_100:  22.61, episodes: 41\n",
      "959: reward:  32.00, mean_100:  22.83, episodes: 42\n",
      "989: reward:  30.00, mean_100:  23.00, episodes: 43\n",
      "1007: reward:  18.00, mean_100:  22.89, episodes: 44\n",
      "1028: reward:  21.00, mean_100:  22.84, episodes: 45\n",
      "1080: reward:  52.00, mean_100:  23.48, episodes: 46\n",
      "1130: reward:  50.00, mean_100:  24.04, episodes: 47\n",
      "1154: reward:  24.00, mean_100:  24.04, episodes: 48\n",
      "1179: reward:  25.00, mean_100:  24.06, episodes: 49\n",
      "1202: reward:  23.00, mean_100:  24.04, episodes: 50\n",
      "1265: reward:  63.00, mean_100:  24.80, episodes: 51\n",
      "1277: reward:  12.00, mean_100:  24.56, episodes: 52\n",
      "1304: reward:  27.00, mean_100:  24.60, episodes: 53\n",
      "1321: reward:  17.00, mean_100:  24.46, episodes: 54\n",
      "1383: reward:  62.00, mean_100:  25.15, episodes: 55\n",
      "1399: reward:  16.00, mean_100:  24.98, episodes: 56\n",
      "1418: reward:  19.00, mean_100:  24.88, episodes: 57\n",
      "1463: reward:  45.00, mean_100:  25.22, episodes: 58\n",
      "1503: reward:  40.00, mean_100:  25.47, episodes: 59\n",
      "1548: reward:  45.00, mean_100:  25.80, episodes: 60\n",
      "1578: reward:  30.00, mean_100:  25.87, episodes: 61\n",
      "1593: reward:  15.00, mean_100:  25.69, episodes: 62\n",
      "1613: reward:  20.00, mean_100:  25.60, episodes: 63\n",
      "1633: reward:  20.00, mean_100:  25.52, episodes: 64\n",
      "1666: reward:  33.00, mean_100:  25.63, episodes: 65\n",
      "1713: reward:  47.00, mean_100:  25.95, episodes: 66\n",
      "1729: reward:  16.00, mean_100:  25.81, episodes: 67\n",
      "1744: reward:  15.00, mean_100:  25.65, episodes: 68\n",
      "1814: reward:  70.00, mean_100:  26.29, episodes: 69\n",
      "1835: reward:  21.00, mean_100:  26.21, episodes: 70\n",
      "1863: reward:  28.00, mean_100:  26.24, episodes: 71\n",
      "1927: reward:  64.00, mean_100:  26.76, episodes: 72\n",
      "2038: reward: 111.00, mean_100:  27.92, episodes: 73\n",
      "2133: reward:  95.00, mean_100:  28.82, episodes: 74\n",
      "2246: reward: 113.00, mean_100:  29.95, episodes: 75\n",
      "2270: reward:  24.00, mean_100:  29.87, episodes: 76\n",
      "2314: reward:  44.00, mean_100:  30.05, episodes: 77\n",
      "2346: reward:  32.00, mean_100:  30.08, episodes: 78\n",
      "2417: reward:  71.00, mean_100:  30.59, episodes: 79\n",
      "2491: reward:  74.00, mean_100:  31.14, episodes: 80\n",
      "2558: reward:  67.00, mean_100:  31.58, episodes: 81\n",
      "2577: reward:  19.00, mean_100:  31.43, episodes: 82\n",
      "2601: reward:  24.00, mean_100:  31.34, episodes: 83\n",
      "2681: reward:  80.00, mean_100:  31.92, episodes: 84\n",
      "2769: reward:  88.00, mean_100:  32.58, episodes: 85\n",
      "2879: reward: 110.00, mean_100:  33.48, episodes: 86\n",
      "2927: reward:  48.00, mean_100:  33.64, episodes: 87\n",
      "2970: reward:  43.00, mean_100:  33.75, episodes: 88\n",
      "3119: reward: 149.00, mean_100:  35.04, episodes: 89\n",
      "3129: reward:  10.00, mean_100:  34.77, episodes: 90\n",
      "3331: reward: 202.00, mean_100:  36.60, episodes: 91\n",
      "3365: reward:  34.00, mean_100:  36.58, episodes: 92\n",
      "3403: reward:  38.00, mean_100:  36.59, episodes: 93\n",
      "3479: reward:  76.00, mean_100:  37.01, episodes: 94\n",
      "3574: reward:  95.00, mean_100:  37.62, episodes: 95\n",
      "3656: reward:  82.00, mean_100:  38.08, episodes: 96\n",
      "3715: reward:  59.00, mean_100:  38.30, episodes: 97\n",
      "3780: reward:  65.00, mean_100:  38.57, episodes: 98\n",
      "3846: reward:  66.00, mean_100:  38.85, episodes: 99\n",
      "3936: reward:  90.00, mean_100:  39.36, episodes: 100\n",
      "4033: reward:  97.00, mean_100:  40.04, episodes: 101\n",
      "4155: reward: 122.00, mean_100:  41.03, episodes: 102\n",
      "4246: reward:  91.00, mean_100:  41.83, episodes: 103\n",
      "4340: reward:  94.00, mean_100:  42.60, episodes: 104\n",
      "4403: reward:  63.00, mean_100:  43.11, episodes: 105\n",
      "4475: reward:  72.00, mean_100:  43.54, episodes: 106\n",
      "4546: reward:  71.00, mean_100:  44.11, episodes: 107\n",
      "4631: reward:  85.00, mean_100:  44.81, episodes: 108\n",
      "4814: reward: 183.00, mean_100:  46.34, episodes: 109\n",
      "4909: reward:  95.00, mean_100:  47.14, episodes: 110\n",
      "4971: reward:  62.00, mean_100:  47.44, episodes: 111\n",
      "5001: reward:  30.00, mean_100:  47.60, episodes: 112\n",
      "5038: reward:  37.00, mean_100:  47.78, episodes: 113\n",
      "5266: reward: 228.00, mean_100:  49.86, episodes: 114\n",
      "5377: reward: 111.00, mean_100:  50.82, episodes: 115\n",
      "5461: reward:  84.00, mean_100:  51.42, episodes: 116\n",
      "5493: reward:  32.00, mean_100:  51.32, episodes: 117\n",
      "5546: reward:  53.00, mean_100:  51.61, episodes: 118\n",
      "5705: reward: 159.00, mean_100:  53.07, episodes: 119\n",
      "5832: reward: 127.00, mean_100:  54.17, episodes: 120\n",
      "6051: reward: 219.00, mean_100:  56.27, episodes: 121\n",
      "6168: reward: 117.00, mean_100:  57.07, episodes: 122\n",
      "6317: reward: 149.00, mean_100:  58.39, episodes: 123\n",
      "6464: reward: 147.00, mean_100:  59.56, episodes: 124\n",
      "6548: reward:  84.00, mean_100:  59.91, episodes: 125\n",
      "6838: reward: 290.00, mean_100:  62.60, episodes: 126\n",
      "6983: reward: 145.00, mean_100:  63.88, episodes: 127\n",
      "7040: reward:  57.00, mean_100:  64.07, episodes: 128\n",
      "7170: reward: 130.00, mean_100:  65.21, episodes: 129\n",
      "7289: reward: 119.00, mean_100:  66.25, episodes: 130\n",
      "7397: reward: 108.00, mean_100:  67.09, episodes: 131\n",
      "7512: reward: 115.00, mean_100:  68.06, episodes: 132\n",
      "7616: reward: 104.00, mean_100:  69.01, episodes: 133\n",
      "7803: reward: 187.00, mean_100:  70.55, episodes: 134\n",
      "7842: reward:  39.00, mean_100:  70.78, episodes: 135\n",
      "8092: reward: 250.00, mean_100:  73.02, episodes: 136\n",
      "8179: reward:  87.00, mean_100:  73.39, episodes: 137\n",
      "8426: reward: 247.00, mean_100:  75.46, episodes: 138\n",
      "8562: reward: 136.00, mean_100:  76.66, episodes: 139\n",
      "8766: reward: 204.00, mean_100:  78.59, episodes: 140\n",
      "9008: reward: 242.00, mean_100:  80.81, episodes: 141\n",
      "9232: reward: 224.00, mean_100:  82.73, episodes: 142\n",
      "9443: reward: 211.00, mean_100:  84.54, episodes: 143\n",
      "9687: reward: 244.00, mean_100:  86.80, episodes: 144\n",
      "9737: reward:  50.00, mean_100:  87.09, episodes: 145\n",
      "9978: reward: 241.00, mean_100:  88.98, episodes: 146\n",
      "10139: reward: 161.00, mean_100:  90.09, episodes: 147\n",
      "10208: reward:  69.00, mean_100:  90.54, episodes: 148\n",
      "10455: reward: 247.00, mean_100:  92.76, episodes: 149\n",
      "10595: reward: 140.00, mean_100:  93.93, episodes: 150\n",
      "10704: reward: 109.00, mean_100:  94.39, episodes: 151\n",
      "10863: reward: 159.00, mean_100:  95.86, episodes: 152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11149: reward: 286.00, mean_100:  98.45, episodes: 153\n",
      "11381: reward: 232.00, mean_100: 100.60, episodes: 154\n",
      "11689: reward: 308.00, mean_100: 103.06, episodes: 155\n",
      "12108: reward: 419.00, mean_100: 107.09, episodes: 156\n",
      "12264: reward: 156.00, mean_100: 108.46, episodes: 157\n",
      "12612: reward: 348.00, mean_100: 111.49, episodes: 158\n",
      "12855: reward: 243.00, mean_100: 113.52, episodes: 159\n",
      "13158: reward: 303.00, mean_100: 116.10, episodes: 160\n",
      "13368: reward: 210.00, mean_100: 117.90, episodes: 161\n",
      "13408: reward:  40.00, mean_100: 118.15, episodes: 162\n",
      "13556: reward: 148.00, mean_100: 119.43, episodes: 163\n",
      "13685: reward: 129.00, mean_100: 120.52, episodes: 164\n",
      "14007: reward: 322.00, mean_100: 123.41, episodes: 165\n",
      "14106: reward:  99.00, mean_100: 123.93, episodes: 166\n",
      "14483: reward: 377.00, mean_100: 127.54, episodes: 167\n",
      "14843: reward: 360.00, mean_100: 130.99, episodes: 168\n",
      "15343: reward: 500.00, mean_100: 135.29, episodes: 169\n",
      "15826: reward: 483.00, mean_100: 139.91, episodes: 170\n",
      "16138: reward: 312.00, mean_100: 142.75, episodes: 171\n",
      "16197: reward:  59.00, mean_100: 142.70, episodes: 172\n",
      "16697: reward: 500.00, mean_100: 146.59, episodes: 173\n",
      "17197: reward: 500.00, mean_100: 150.64, episodes: 174\n",
      "17568: reward: 371.00, mean_100: 153.22, episodes: 175\n",
      "17801: reward: 233.00, mean_100: 155.31, episodes: 176\n",
      "18130: reward: 329.00, mean_100: 158.16, episodes: 177\n",
      "18329: reward: 199.00, mean_100: 159.83, episodes: 178\n",
      "18495: reward: 166.00, mean_100: 160.78, episodes: 179\n",
      "18648: reward: 153.00, mean_100: 161.57, episodes: 180\n",
      "18785: reward: 137.00, mean_100: 162.27, episodes: 181\n",
      "18881: reward:  96.00, mean_100: 163.04, episodes: 182\n",
      "19029: reward: 148.00, mean_100: 164.28, episodes: 183\n",
      "19146: reward: 117.00, mean_100: 164.65, episodes: 184\n",
      "19298: reward: 152.00, mean_100: 165.29, episodes: 185\n",
      "19428: reward: 130.00, mean_100: 165.49, episodes: 186\n",
      "19541: reward: 113.00, mean_100: 166.14, episodes: 187\n",
      "19666: reward: 125.00, mean_100: 166.96, episodes: 188\n",
      "19829: reward: 163.00, mean_100: 167.10, episodes: 189\n",
      "19989: reward: 160.00, mean_100: 168.60, episodes: 190\n",
      "20151: reward: 162.00, mean_100: 168.20, episodes: 191\n",
      "20336: reward: 185.00, mean_100: 169.71, episodes: 192\n",
      "20658: reward: 322.00, mean_100: 172.55, episodes: 193\n",
      "20885: reward: 227.00, mean_100: 174.06, episodes: 194\n",
      "21082: reward: 197.00, mean_100: 175.08, episodes: 195\n",
      "21231: reward: 149.00, mean_100: 175.75, episodes: 196\n",
      "21425: reward: 194.00, mean_100: 177.10, episodes: 197\n",
      "21568: reward: 143.00, mean_100: 177.88, episodes: 198\n",
      "21615: reward:  47.00, mean_100: 177.69, episodes: 199\n",
      "21761: reward: 146.00, mean_100: 178.25, episodes: 200\n",
      "21889: reward: 128.00, mean_100: 178.56, episodes: 201\n",
      "22035: reward: 146.00, mean_100: 178.80, episodes: 202\n",
      "22179: reward: 144.00, mean_100: 179.33, episodes: 203\n",
      "22318: reward: 139.00, mean_100: 179.78, episodes: 204\n",
      "22448: reward: 130.00, mean_100: 180.45, episodes: 205\n",
      "22644: reward: 196.00, mean_100: 181.69, episodes: 206\n",
      "22850: reward: 206.00, mean_100: 183.04, episodes: 207\n",
      "23023: reward: 173.00, mean_100: 183.92, episodes: 208\n",
      "23188: reward: 165.00, mean_100: 183.74, episodes: 209\n",
      "23401: reward: 213.00, mean_100: 184.92, episodes: 210\n",
      "23559: reward: 158.00, mean_100: 185.88, episodes: 211\n",
      "23727: reward: 168.00, mean_100: 187.26, episodes: 212\n",
      "23893: reward: 166.00, mean_100: 188.55, episodes: 213\n",
      "24037: reward: 144.00, mean_100: 187.71, episodes: 214\n",
      "24188: reward: 151.00, mean_100: 188.11, episodes: 215\n",
      "24352: reward: 164.00, mean_100: 188.91, episodes: 216\n",
      "24481: reward: 129.00, mean_100: 189.88, episodes: 217\n",
      "24599: reward: 118.00, mean_100: 190.53, episodes: 218\n",
      "24725: reward: 126.00, mean_100: 190.20, episodes: 219\n",
      "24839: reward: 114.00, mean_100: 190.07, episodes: 220\n",
      "24957: reward: 118.00, mean_100: 189.06, episodes: 221\n",
      "25115: reward: 158.00, mean_100: 189.47, episodes: 222\n",
      "25235: reward: 120.00, mean_100: 189.18, episodes: 223\n",
      "25413: reward: 178.00, mean_100: 189.49, episodes: 224\n",
      "25541: reward: 128.00, mean_100: 189.93, episodes: 225\n",
      "25720: reward: 179.00, mean_100: 188.82, episodes: 226\n",
      "25878: reward: 158.00, mean_100: 188.95, episodes: 227\n",
      "25989: reward: 111.00, mean_100: 189.49, episodes: 228\n",
      "26138: reward: 149.00, mean_100: 189.68, episodes: 229\n",
      "26257: reward: 119.00, mean_100: 189.68, episodes: 230\n",
      "26397: reward: 140.00, mean_100: 190.00, episodes: 231\n",
      "26507: reward: 110.00, mean_100: 189.95, episodes: 232\n",
      "26628: reward: 121.00, mean_100: 190.12, episodes: 233\n",
      "26746: reward: 118.00, mean_100: 189.43, episodes: 234\n",
      "26885: reward: 139.00, mean_100: 190.43, episodes: 235\n",
      "27058: reward: 173.00, mean_100: 189.66, episodes: 236\n",
      "27221: reward: 163.00, mean_100: 190.42, episodes: 237\n",
      "27370: reward: 149.00, mean_100: 189.44, episodes: 238\n",
      "27542: reward: 172.00, mean_100: 189.80, episodes: 239\n",
      "27718: reward: 176.00, mean_100: 189.52, episodes: 240\n",
      "27840: reward: 122.00, mean_100: 188.32, episodes: 241\n",
      "28007: reward: 167.00, mean_100: 187.75, episodes: 242\n",
      "28266: reward: 259.00, mean_100: 188.23, episodes: 243\n",
      "28600: reward: 334.00, mean_100: 189.13, episodes: 244\n",
      "28826: reward: 226.00, mean_100: 190.89, episodes: 245\n",
      "29074: reward: 248.00, mean_100: 190.96, episodes: 246\n",
      "29574: reward: 500.00, mean_100: 194.35, episodes: 247\n",
      "29932: reward: 358.00, mean_100: 197.24, episodes: 248\n",
      "Solved in 29932 steps and 248 episodes!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    rew=[]\n",
    "    ep=[]\n",
    "    \n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    env = gym.wrappers.Monitor(env, \"recording\",force=True)\n",
    "    writer = SummaryWriter(comment=\"-cartpole-pg\")\n",
    "\n",
    "    net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "    print(net)\n",
    "\n",
    "    agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor,\n",
    "                                   apply_softmax=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #source is asked to unroll the Bellman equation for 10 steps\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    total_rewards = []\n",
    "    step_rewards = []\n",
    "    step_idx = 0\n",
    "    done_episodes = 0\n",
    "    reward_sum = 0.0\n",
    "\n",
    "    batch_states, batch_actions, batch_scales = [], [], []\n",
    "    \n",
    "    '''In the training loop, we maintain the sum of the discounted reward for every\n",
    "transition and use it to calculate the baseline for policy scale.'''\n",
    "\n",
    "    for step_idx, exp in enumerate(exp_source):\n",
    "        reward_sum += exp.reward\n",
    "        baseline = reward_sum / (step_idx + 1)\n",
    "        writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "        batch_states.append(exp.state)\n",
    "        batch_actions.append(int(exp.action))\n",
    "        batch_scales.append(exp.reward - baseline)\n",
    "\n",
    "        # handle new rewards\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "            done_episodes += 1\n",
    "            reward = new_rewards[0]\n",
    "            total_rewards.append(reward)\n",
    "            mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "            print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\" % (\n",
    "                step_idx, reward, mean_rewards, done_episodes))\n",
    "            ep.append(done_episodes)\n",
    "            rew.append(mean_rewards)\n",
    "            writer.add_scalar(\"reward\", reward, step_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "            writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "            if mean_rewards > 195:\n",
    "                print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n",
    "                break\n",
    "\n",
    "        if len(batch_states) < BATCH_SIZE:\n",
    "            continue\n",
    "\n",
    "        states_v = torch.FloatTensor(batch_states)\n",
    "        batch_actions_t = torch.LongTensor(batch_actions)\n",
    "        batch_scale_v = torch.FloatTensor(batch_scales)\n",
    "        \n",
    "        '''Then we add the entropy bonus to the loss by calculating the entropy of the\n",
    "batch and subtracting it from the loss. As entropy has a maximum for uniform\n",
    "probability distribution and we want to push the training towards this maximum,\n",
    "we need to subtract from the loss'''\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits_v = net(states_v)\n",
    "        log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "        log_prob_actions_v = batch_scale_v * log_prob_v[range(BATCH_SIZE), batch_actions_t]\n",
    "        loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        prob_v = F.softmax(logits_v, dim=1)\n",
    "        entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean()\n",
    "        entropy_loss_v = -ENTROPY_BETA * entropy_v\n",
    "        loss_v = loss_policy_v + entropy_loss_v\n",
    "\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calc KL-div\n",
    "        '''Then, we calculate the Kullback-Leibler (KL)-divergence between the new and\n",
    "the old policy. KL-divergence is an information theory measurement of how one\n",
    "probability distribution diverges from another expected probability distribution.\n",
    "In our example, it is being used to compare the policy returned by the model\n",
    "before and after the optimization step. High spikes in KL are usually a bad sign,\n",
    "showing that our policy was pushed too far from the previous policy, which is a\n",
    "bad idea most of the time (as our NN is a very nonlinear function in a high-dimension space, so large changes in the model weight could have a very strong\n",
    "influence on policy)'''\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        new_logits_v = net(states_v)\n",
    "        new_prob_v = F.softmax(new_logits_v, dim=1)\n",
    "        kl_div_v = -((new_prob_v / prob_v).log() * prob_v).sum(dim=1).mean()\n",
    "        writer.add_scalar(\"kl\", kl_div_v.item(), step_idx)\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''Finally, we calculate the statistics about the gradients on this training step. It’s\n",
    "usually good practice to show the graph of maximum and L2-norm of gradients\n",
    "to get an idea about the training dynamics.'''\n",
    "\n",
    "        grad_max = 0.0\n",
    "        grad_means = 0.0\n",
    "        grad_count = 0\n",
    "        for p in net.parameters():\n",
    "            grad_max = max(grad_max, p.grad.abs().max().item())\n",
    "            grad_means += (p.grad ** 2).mean().sqrt().item()\n",
    "            grad_count += 1\n",
    "\n",
    "            \n",
    "        '''summary dropped in tensorboard'''\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "        writer.add_scalar(\"entropy\", entropy_v.item(), step_idx)\n",
    "        writer.add_scalar(\"batch_scales\", np.mean(batch_scales), step_idx)\n",
    "        writer.add_scalar(\"loss_entropy\", entropy_loss_v.item(), step_idx)\n",
    "        writer.add_scalar(\"loss_policy\", loss_policy_v.item(), step_idx)\n",
    "        writer.add_scalar(\"loss_total\", loss_v.item(), step_idx)\n",
    "        writer.add_scalar(\"grad_l2\", grad_means / grad_count, step_idx)\n",
    "        writer.add_scalar(\"grad_max\", grad_max, step_idx)\n",
    "\n",
    "        batch_states.clear()\n",
    "        batch_actions.clear()\n",
    "        batch_scales.clear()\n",
    "    env.close()\n",
    "    env.env.close()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
