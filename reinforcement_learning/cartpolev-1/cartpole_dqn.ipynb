{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setting is same as Problem 2. However, you have to implement a neural network based Q-\n",
    "learning solution rather than implementing a policy gradients solution. You are required to experiment with\n",
    "only the CartPole-v1 environment. Answer all the questions that have been asked in Problem 2 for this\n",
    "setting. The plot must be named as “bonus.png”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_STOP = 0.02\n",
    "EPSILON_STEPS = 5000\n",
    "\n",
    "REPLAY_BUFFER = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining the neural network for the Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the target state based on the best Qvalue. The net returns the action value of all actions\n",
    "in action space.If we are done then we return local reward, else we go for bellman update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_target(net, local_reward, next_state):\n",
    "    if next_state is None:\n",
    "        return local_reward\n",
    "    state_v = torch.tensor([next_state], dtype=torch.float32)\n",
    "    next_q_v = net(state_v)\n",
    "    best_q = next_q_v.max(dim=1)[0].item()\n",
    "    return local_reward + GAMMA * best_q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "10: reward:   9.00, mean_100:   9.00, epsilon: 1.00, episodes: 1\n",
      "41: reward:  31.00, mean_100:  20.00, epsilon: 0.99, episodes: 2\n",
      "58: reward:  17.00, mean_100:  19.00, epsilon: 0.99, episodes: 3\n",
      "68: reward:  10.00, mean_100:  16.75, epsilon: 0.99, episodes: 4\n",
      "101: reward:  33.00, mean_100:  20.00, epsilon: 0.98, episodes: 5\n",
      "128: reward:  27.00, mean_100:  21.17, epsilon: 0.97, episodes: 6\n",
      "145: reward:  17.00, mean_100:  20.57, epsilon: 0.97, episodes: 7\n",
      "166: reward:  21.00, mean_100:  20.62, epsilon: 0.97, episodes: 8\n",
      "181: reward:  15.00, mean_100:  20.00, epsilon: 0.96, episodes: 9\n",
      "203: reward:  22.00, mean_100:  20.20, epsilon: 0.96, episodes: 10\n",
      "250: reward:  47.00, mean_100:  22.64, epsilon: 0.95, episodes: 11\n",
      "263: reward:  13.00, mean_100:  21.83, epsilon: 0.95, episodes: 12\n",
      "282: reward:  19.00, mean_100:  21.62, epsilon: 0.94, episodes: 13\n",
      "331: reward:  49.00, mean_100:  23.57, epsilon: 0.93, episodes: 14\n",
      "347: reward:  16.00, mean_100:  23.07, epsilon: 0.93, episodes: 15\n",
      "364: reward:  17.00, mean_100:  22.69, epsilon: 0.93, episodes: 16\n",
      "387: reward:  23.00, mean_100:  22.71, epsilon: 0.92, episodes: 17\n",
      "414: reward:  27.00, mean_100:  22.94, epsilon: 0.92, episodes: 18\n",
      "441: reward:  27.00, mean_100:  23.16, epsilon: 0.91, episodes: 19\n",
      "461: reward:  20.00, mean_100:  23.00, epsilon: 0.91, episodes: 20\n",
      "481: reward:  20.00, mean_100:  22.86, epsilon: 0.90, episodes: 21\n",
      "506: reward:  25.00, mean_100:  22.95, epsilon: 0.90, episodes: 22\n",
      "527: reward:  21.00, mean_100:  22.87, epsilon: 0.89, episodes: 23\n",
      "554: reward:  27.00, mean_100:  23.04, epsilon: 0.89, episodes: 24\n",
      "578: reward:  24.00, mean_100:  23.08, epsilon: 0.88, episodes: 25\n",
      "590: reward:  12.00, mean_100:  22.65, epsilon: 0.88, episodes: 26\n",
      "611: reward:  21.00, mean_100:  22.59, epsilon: 0.88, episodes: 27\n",
      "622: reward:  11.00, mean_100:  22.18, epsilon: 0.88, episodes: 28\n",
      "635: reward:  13.00, mean_100:  21.86, epsilon: 0.87, episodes: 29\n",
      "658: reward:  23.00, mean_100:  21.90, epsilon: 0.87, episodes: 30\n",
      "676: reward:  18.00, mean_100:  21.77, epsilon: 0.86, episodes: 31\n",
      "709: reward:  33.00, mean_100:  22.12, epsilon: 0.86, episodes: 32\n",
      "736: reward:  27.00, mean_100:  22.27, epsilon: 0.85, episodes: 33\n",
      "757: reward:  21.00, mean_100:  22.24, epsilon: 0.85, episodes: 34\n",
      "770: reward:  13.00, mean_100:  21.97, epsilon: 0.85, episodes: 35\n",
      "782: reward:  12.00, mean_100:  21.69, epsilon: 0.84, episodes: 36\n",
      "795: reward:  13.00, mean_100:  21.46, epsilon: 0.84, episodes: 37\n",
      "810: reward:  15.00, mean_100:  21.29, epsilon: 0.84, episodes: 38\n",
      "842: reward:  32.00, mean_100:  21.56, epsilon: 0.83, episodes: 39\n",
      "855: reward:  13.00, mean_100:  21.35, epsilon: 0.83, episodes: 40\n",
      "869: reward:  14.00, mean_100:  21.17, epsilon: 0.83, episodes: 41\n",
      "885: reward:  16.00, mean_100:  21.05, epsilon: 0.82, episodes: 42\n",
      "895: reward:  10.00, mean_100:  20.79, epsilon: 0.82, episodes: 43\n",
      "911: reward:  16.00, mean_100:  20.68, epsilon: 0.82, episodes: 44\n",
      "923: reward:  12.00, mean_100:  20.49, epsilon: 0.82, episodes: 45\n",
      "934: reward:  11.00, mean_100:  20.28, epsilon: 0.81, episodes: 46\n",
      "948: reward:  14.00, mean_100:  20.15, epsilon: 0.81, episodes: 47\n",
      "966: reward:  18.00, mean_100:  20.10, epsilon: 0.81, episodes: 48\n",
      "999: reward:  33.00, mean_100:  20.37, epsilon: 0.80, episodes: 49\n",
      "1017: reward:  18.00, mean_100:  20.32, epsilon: 0.80, episodes: 50\n",
      "1031: reward:  14.00, mean_100:  20.20, epsilon: 0.79, episodes: 51\n",
      "1066: reward:  35.00, mean_100:  20.48, epsilon: 0.79, episodes: 52\n",
      "1080: reward:  14.00, mean_100:  20.36, epsilon: 0.78, episodes: 53\n",
      "1093: reward:  13.00, mean_100:  20.22, epsilon: 0.78, episodes: 54\n",
      "1102: reward:   9.00, mean_100:  20.02, epsilon: 0.78, episodes: 55\n",
      "1115: reward:  13.00, mean_100:  19.89, epsilon: 0.78, episodes: 56\n",
      "1142: reward:  27.00, mean_100:  20.02, epsilon: 0.77, episodes: 57\n",
      "1153: reward:  11.00, mean_100:  19.86, epsilon: 0.77, episodes: 58\n",
      "1168: reward:  15.00, mean_100:  19.78, epsilon: 0.77, episodes: 59\n",
      "1186: reward:  18.00, mean_100:  19.75, epsilon: 0.76, episodes: 60\n",
      "1208: reward:  22.00, mean_100:  19.79, epsilon: 0.76, episodes: 61\n",
      "1229: reward:  21.00, mean_100:  19.81, epsilon: 0.75, episodes: 62\n",
      "1258: reward:  29.00, mean_100:  19.95, epsilon: 0.75, episodes: 63\n",
      "1274: reward:  16.00, mean_100:  19.89, epsilon: 0.75, episodes: 64\n",
      "1290: reward:  16.00, mean_100:  19.83, epsilon: 0.74, episodes: 65\n",
      "1304: reward:  14.00, mean_100:  19.74, epsilon: 0.74, episodes: 66\n",
      "1347: reward:  43.00, mean_100:  20.09, epsilon: 0.73, episodes: 67\n",
      "1357: reward:  10.00, mean_100:  19.94, epsilon: 0.73, episodes: 68\n",
      "1379: reward:  22.00, mean_100:  19.97, epsilon: 0.72, episodes: 69\n",
      "1398: reward:  19.00, mean_100:  19.96, epsilon: 0.72, episodes: 70\n",
      "1410: reward:  12.00, mean_100:  19.85, epsilon: 0.72, episodes: 71\n",
      "1424: reward:  14.00, mean_100:  19.76, epsilon: 0.72, episodes: 72\n",
      "1449: reward:  25.00, mean_100:  19.84, epsilon: 0.71, episodes: 73\n",
      "1475: reward:  26.00, mean_100:  19.92, epsilon: 0.71, episodes: 74\n",
      "1492: reward:  17.00, mean_100:  19.88, epsilon: 0.70, episodes: 75\n",
      "1505: reward:  13.00, mean_100:  19.79, epsilon: 0.70, episodes: 76\n",
      "1516: reward:  11.00, mean_100:  19.68, epsilon: 0.70, episodes: 77\n",
      "1525: reward:   9.00, mean_100:  19.54, epsilon: 0.70, episodes: 78\n",
      "1546: reward:  21.00, mean_100:  19.56, epsilon: 0.69, episodes: 79\n",
      "1554: reward:   8.00, mean_100:  19.41, epsilon: 0.69, episodes: 80\n",
      "1578: reward:  24.00, mean_100:  19.47, epsilon: 0.68, episodes: 81\n",
      "1589: reward:  11.00, mean_100:  19.37, epsilon: 0.68, episodes: 82\n",
      "1603: reward:  14.00, mean_100:  19.30, epsilon: 0.68, episodes: 83\n",
      "1621: reward:  18.00, mean_100:  19.29, epsilon: 0.68, episodes: 84\n",
      "1641: reward:  20.00, mean_100:  19.29, epsilon: 0.67, episodes: 85\n",
      "1651: reward:  10.00, mean_100:  19.19, epsilon: 0.67, episodes: 86\n",
      "1672: reward:  21.00, mean_100:  19.21, epsilon: 0.67, episodes: 87\n",
      "1686: reward:  14.00, mean_100:  19.15, epsilon: 0.66, episodes: 88\n",
      "1695: reward:   9.00, mean_100:  19.03, epsilon: 0.66, episodes: 89\n",
      "1707: reward:  12.00, mean_100:  18.96, epsilon: 0.66, episodes: 90\n",
      "1719: reward:  12.00, mean_100:  18.88, epsilon: 0.66, episodes: 91\n",
      "1735: reward:  16.00, mean_100:  18.85, epsilon: 0.65, episodes: 92\n",
      "1751: reward:  16.00, mean_100:  18.82, epsilon: 0.65, episodes: 93\n",
      "1766: reward:  15.00, mean_100:  18.78, epsilon: 0.65, episodes: 94\n",
      "1779: reward:  13.00, mean_100:  18.72, epsilon: 0.64, episodes: 95\n",
      "1795: reward:  16.00, mean_100:  18.69, epsilon: 0.64, episodes: 96\n",
      "1832: reward:  37.00, mean_100:  18.88, epsilon: 0.63, episodes: 97\n",
      "1883: reward:  51.00, mean_100:  19.20, epsilon: 0.62, episodes: 98\n",
      "1901: reward:  18.00, mean_100:  19.19, epsilon: 0.62, episodes: 99\n",
      "1917: reward:  16.00, mean_100:  19.16, epsilon: 0.62, episodes: 100\n",
      "1931: reward:  14.00, mean_100:  19.21, epsilon: 0.61, episodes: 101\n",
      "1951: reward:  20.00, mean_100:  19.10, epsilon: 0.61, episodes: 102\n",
      "1965: reward:  14.00, mean_100:  19.07, epsilon: 0.61, episodes: 103\n",
      "1990: reward:  25.00, mean_100:  19.22, epsilon: 0.60, episodes: 104\n",
      "2003: reward:  13.00, mean_100:  19.02, epsilon: 0.60, episodes: 105\n",
      "2013: reward:  10.00, mean_100:  18.85, epsilon: 0.60, episodes: 106\n",
      "2026: reward:  13.00, mean_100:  18.81, epsilon: 0.59, episodes: 107\n",
      "2050: reward:  24.00, mean_100:  18.84, epsilon: 0.59, episodes: 108\n",
      "2062: reward:  12.00, mean_100:  18.81, epsilon: 0.59, episodes: 109\n",
      "2074: reward:  12.00, mean_100:  18.71, epsilon: 0.59, episodes: 110\n",
      "2098: reward:  24.00, mean_100:  18.48, epsilon: 0.58, episodes: 111\n",
      "2112: reward:  14.00, mean_100:  18.49, epsilon: 0.58, episodes: 112\n",
      "2135: reward:  23.00, mean_100:  18.53, epsilon: 0.57, episodes: 113\n",
      "2149: reward:  14.00, mean_100:  18.18, epsilon: 0.57, episodes: 114\n",
      "2162: reward:  13.00, mean_100:  18.15, epsilon: 0.57, episodes: 115\n",
      "2176: reward:  14.00, mean_100:  18.12, epsilon: 0.56, episodes: 116\n",
      "2186: reward:  10.00, mean_100:  17.99, epsilon: 0.56, episodes: 117\n",
      "2205: reward:  19.00, mean_100:  17.91, epsilon: 0.56, episodes: 118\n",
      "2237: reward:  32.00, mean_100:  17.96, epsilon: 0.55, episodes: 119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2257: reward:  20.00, mean_100:  17.96, epsilon: 0.55, episodes: 120\n",
      "2269: reward:  12.00, mean_100:  17.88, epsilon: 0.55, episodes: 121\n",
      "2278: reward:   9.00, mean_100:  17.72, epsilon: 0.54, episodes: 122\n",
      "2305: reward:  27.00, mean_100:  17.78, epsilon: 0.54, episodes: 123\n",
      "2327: reward:  22.00, mean_100:  17.73, epsilon: 0.53, episodes: 124\n",
      "2348: reward:  21.00, mean_100:  17.70, epsilon: 0.53, episodes: 125\n",
      "2374: reward:  26.00, mean_100:  17.84, epsilon: 0.53, episodes: 126\n",
      "2384: reward:  10.00, mean_100:  17.73, epsilon: 0.52, episodes: 127\n",
      "2396: reward:  12.00, mean_100:  17.74, epsilon: 0.52, episodes: 128\n",
      "2407: reward:  11.00, mean_100:  17.72, epsilon: 0.52, episodes: 129\n",
      "2426: reward:  19.00, mean_100:  17.68, epsilon: 0.51, episodes: 130\n",
      "2437: reward:  11.00, mean_100:  17.61, epsilon: 0.51, episodes: 131\n",
      "2457: reward:  20.00, mean_100:  17.48, epsilon: 0.51, episodes: 132\n",
      "2467: reward:  10.00, mean_100:  17.31, epsilon: 0.51, episodes: 133\n",
      "2477: reward:  10.00, mean_100:  17.20, epsilon: 0.50, episodes: 134\n",
      "2490: reward:  13.00, mean_100:  17.20, epsilon: 0.50, episodes: 135\n",
      "2502: reward:  12.00, mean_100:  17.20, epsilon: 0.50, episodes: 136\n",
      "2518: reward:  16.00, mean_100:  17.23, epsilon: 0.50, episodes: 137\n",
      "2530: reward:  12.00, mean_100:  17.20, epsilon: 0.49, episodes: 138\n",
      "2558: reward:  28.00, mean_100:  17.16, epsilon: 0.49, episodes: 139\n",
      "2568: reward:  10.00, mean_100:  17.13, epsilon: 0.49, episodes: 140\n",
      "2587: reward:  19.00, mean_100:  17.18, epsilon: 0.48, episodes: 141\n",
      "2598: reward:  11.00, mean_100:  17.13, epsilon: 0.48, episodes: 142\n",
      "2610: reward:  12.00, mean_100:  17.15, epsilon: 0.48, episodes: 143\n",
      "2618: reward:   8.00, mean_100:  17.07, epsilon: 0.48, episodes: 144\n",
      "2631: reward:  13.00, mean_100:  17.08, epsilon: 0.47, episodes: 145\n",
      "2642: reward:  11.00, mean_100:  17.08, epsilon: 0.47, episodes: 146\n",
      "2654: reward:  12.00, mean_100:  17.06, epsilon: 0.47, episodes: 147\n",
      "2667: reward:  13.00, mean_100:  17.01, epsilon: 0.47, episodes: 148\n",
      "2681: reward:  14.00, mean_100:  16.82, epsilon: 0.46, episodes: 149\n",
      "2692: reward:  11.00, mean_100:  16.75, epsilon: 0.46, episodes: 150\n",
      "2703: reward:  11.00, mean_100:  16.72, epsilon: 0.46, episodes: 151\n",
      "2736: reward:  33.00, mean_100:  16.70, epsilon: 0.45, episodes: 152\n",
      "2747: reward:  11.00, mean_100:  16.67, epsilon: 0.45, episodes: 153\n",
      "2760: reward:  13.00, mean_100:  16.67, epsilon: 0.45, episodes: 154\n",
      "2775: reward:  15.00, mean_100:  16.73, epsilon: 0.44, episodes: 155\n",
      "2792: reward:  17.00, mean_100:  16.77, epsilon: 0.44, episodes: 156\n",
      "2801: reward:   9.00, mean_100:  16.59, epsilon: 0.44, episodes: 157\n",
      "2811: reward:  10.00, mean_100:  16.58, epsilon: 0.44, episodes: 158\n",
      "2842: reward:  31.00, mean_100:  16.74, epsilon: 0.43, episodes: 159\n",
      "2856: reward:  14.00, mean_100:  16.70, epsilon: 0.43, episodes: 160\n",
      "2866: reward:  10.00, mean_100:  16.58, epsilon: 0.43, episodes: 161\n",
      "2877: reward:  11.00, mean_100:  16.48, epsilon: 0.42, episodes: 162\n",
      "2897: reward:  20.00, mean_100:  16.39, epsilon: 0.42, episodes: 163\n",
      "2923: reward:  26.00, mean_100:  16.49, epsilon: 0.42, episodes: 164\n",
      "2935: reward:  12.00, mean_100:  16.45, epsilon: 0.41, episodes: 165\n",
      "2956: reward:  21.00, mean_100:  16.52, epsilon: 0.41, episodes: 166\n",
      "2974: reward:  18.00, mean_100:  16.27, epsilon: 0.41, episodes: 167\n",
      "2997: reward:  23.00, mean_100:  16.40, epsilon: 0.40, episodes: 168\n",
      "3014: reward:  17.00, mean_100:  16.35, epsilon: 0.40, episodes: 169\n",
      "3034: reward:  20.00, mean_100:  16.36, epsilon: 0.39, episodes: 170\n",
      "3066: reward:  32.00, mean_100:  16.56, epsilon: 0.39, episodes: 171\n",
      "3088: reward:  22.00, mean_100:  16.64, epsilon: 0.38, episodes: 172\n",
      "3124: reward:  36.00, mean_100:  16.75, epsilon: 0.38, episodes: 173\n",
      "3151: reward:  27.00, mean_100:  16.76, epsilon: 0.37, episodes: 174\n",
      "3211: reward:  60.00, mean_100:  17.19, epsilon: 0.36, episodes: 175\n",
      "3237: reward:  26.00, mean_100:  17.32, epsilon: 0.35, episodes: 176\n",
      "3272: reward:  35.00, mean_100:  17.56, epsilon: 0.35, episodes: 177\n",
      "3298: reward:  26.00, mean_100:  17.73, epsilon: 0.34, episodes: 178\n",
      "3328: reward:  30.00, mean_100:  17.82, epsilon: 0.33, episodes: 179\n",
      "3360: reward:  32.00, mean_100:  18.06, epsilon: 0.33, episodes: 180\n",
      "3380: reward:  20.00, mean_100:  18.02, epsilon: 0.32, episodes: 181\n",
      "3461: reward:  81.00, mean_100:  18.72, epsilon: 0.31, episodes: 182\n",
      "3473: reward:  12.00, mean_100:  18.70, epsilon: 0.31, episodes: 183\n",
      "3488: reward:  15.00, mean_100:  18.67, epsilon: 0.30, episodes: 184\n",
      "3505: reward:  17.00, mean_100:  18.64, epsilon: 0.30, episodes: 185\n",
      "3523: reward:  18.00, mean_100:  18.72, epsilon: 0.30, episodes: 186\n",
      "3539: reward:  16.00, mean_100:  18.67, epsilon: 0.29, episodes: 187\n",
      "3555: reward:  16.00, mean_100:  18.69, epsilon: 0.29, episodes: 188\n",
      "3578: reward:  23.00, mean_100:  18.83, epsilon: 0.28, episodes: 189\n",
      "3591: reward:  13.00, mean_100:  18.84, epsilon: 0.28, episodes: 190\n",
      "3606: reward:  15.00, mean_100:  18.87, epsilon: 0.28, episodes: 191\n",
      "3620: reward:  14.00, mean_100:  18.85, epsilon: 0.28, episodes: 192\n",
      "3634: reward:  14.00, mean_100:  18.83, epsilon: 0.27, episodes: 193\n",
      "3650: reward:  16.00, mean_100:  18.84, epsilon: 0.27, episodes: 194\n",
      "3660: reward:  10.00, mean_100:  18.81, epsilon: 0.27, episodes: 195\n",
      "3674: reward:  14.00, mean_100:  18.79, epsilon: 0.27, episodes: 196\n",
      "3687: reward:  13.00, mean_100:  18.55, epsilon: 0.26, episodes: 197\n",
      "3701: reward:  14.00, mean_100:  18.18, epsilon: 0.26, episodes: 198\n",
      "3719: reward:  18.00, mean_100:  18.18, epsilon: 0.26, episodes: 199\n",
      "3731: reward:  12.00, mean_100:  18.14, epsilon: 0.25, episodes: 200\n",
      "3751: reward:  20.00, mean_100:  18.20, epsilon: 0.25, episodes: 201\n",
      "3763: reward:  12.00, mean_100:  18.12, epsilon: 0.25, episodes: 202\n",
      "3777: reward:  14.00, mean_100:  18.12, epsilon: 0.24, episodes: 203\n",
      "3794: reward:  17.00, mean_100:  18.04, epsilon: 0.24, episodes: 204\n",
      "3807: reward:  13.00, mean_100:  18.04, epsilon: 0.24, episodes: 205\n",
      "3821: reward:  14.00, mean_100:  18.08, epsilon: 0.24, episodes: 206\n",
      "3836: reward:  15.00, mean_100:  18.10, epsilon: 0.23, episodes: 207\n",
      "3855: reward:  19.00, mean_100:  18.05, epsilon: 0.23, episodes: 208\n",
      "3874: reward:  19.00, mean_100:  18.12, epsilon: 0.23, episodes: 209\n",
      "3887: reward:  13.00, mean_100:  18.13, epsilon: 0.22, episodes: 210\n",
      "3899: reward:  12.00, mean_100:  18.01, epsilon: 0.22, episodes: 211\n",
      "3920: reward:  21.00, mean_100:  18.08, epsilon: 0.22, episodes: 212\n",
      "3934: reward:  14.00, mean_100:  17.99, epsilon: 0.21, episodes: 213\n",
      "3953: reward:  19.00, mean_100:  18.04, epsilon: 0.21, episodes: 214\n",
      "3976: reward:  23.00, mean_100:  18.14, epsilon: 0.20, episodes: 215\n",
      "3990: reward:  14.00, mean_100:  18.14, epsilon: 0.20, episodes: 216\n",
      "4014: reward:  24.00, mean_100:  18.28, epsilon: 0.20, episodes: 217\n",
      "4091: reward:  77.00, mean_100:  18.86, epsilon: 0.18, episodes: 218\n",
      "4134: reward:  43.00, mean_100:  18.97, epsilon: 0.17, episodes: 219\n",
      "4156: reward:  22.00, mean_100:  18.99, epsilon: 0.17, episodes: 220\n",
      "4183: reward:  27.00, mean_100:  19.14, epsilon: 0.16, episodes: 221\n",
      "4225: reward:  42.00, mean_100:  19.47, epsilon: 0.16, episodes: 222\n",
      "4267: reward:  42.00, mean_100:  19.62, epsilon: 0.15, episodes: 223\n",
      "4318: reward:  51.00, mean_100:  19.91, epsilon: 0.14, episodes: 224\n",
      "4361: reward:  43.00, mean_100:  20.13, epsilon: 0.13, episodes: 225\n",
      "4388: reward:  27.00, mean_100:  20.14, epsilon: 0.12, episodes: 226\n",
      "4434: reward:  46.00, mean_100:  20.50, epsilon: 0.11, episodes: 227\n",
      "4470: reward:  36.00, mean_100:  20.74, epsilon: 0.11, episodes: 228\n",
      "4520: reward:  50.00, mean_100:  21.13, epsilon: 0.10, episodes: 229\n",
      "4567: reward:  47.00, mean_100:  21.41, epsilon: 0.09, episodes: 230\n",
      "4597: reward:  30.00, mean_100:  21.60, epsilon: 0.08, episodes: 231\n",
      "4624: reward:  27.00, mean_100:  21.67, epsilon: 0.08, episodes: 232\n",
      "4665: reward:  41.00, mean_100:  21.98, epsilon: 0.07, episodes: 233\n",
      "4708: reward:  43.00, mean_100:  22.31, epsilon: 0.06, episodes: 234\n",
      "4744: reward:  36.00, mean_100:  22.54, epsilon: 0.05, episodes: 235\n",
      "4790: reward:  46.00, mean_100:  22.88, epsilon: 0.04, episodes: 236\n",
      "4822: reward:  32.00, mean_100:  23.04, epsilon: 0.04, episodes: 237\n",
      "4879: reward:  57.00, mean_100:  23.49, epsilon: 0.02, episodes: 238\n",
      "4917: reward:  38.00, mean_100:  23.59, epsilon: 0.02, episodes: 239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4974: reward:  57.00, mean_100:  24.06, epsilon: 0.02, episodes: 240\n",
      "5012: reward:  38.00, mean_100:  24.25, epsilon: 0.02, episodes: 241\n",
      "5074: reward:  62.00, mean_100:  24.76, epsilon: 0.02, episodes: 242\n",
      "5148: reward:  74.00, mean_100:  25.38, epsilon: 0.02, episodes: 243\n",
      "5184: reward:  36.00, mean_100:  25.66, epsilon: 0.02, episodes: 244\n",
      "5221: reward:  37.00, mean_100:  25.90, epsilon: 0.02, episodes: 245\n",
      "5267: reward:  46.00, mean_100:  26.25, epsilon: 0.02, episodes: 246\n",
      "5317: reward:  50.00, mean_100:  26.63, epsilon: 0.02, episodes: 247\n",
      "5359: reward:  42.00, mean_100:  26.92, epsilon: 0.02, episodes: 248\n",
      "5411: reward:  52.00, mean_100:  27.30, epsilon: 0.02, episodes: 249\n",
      "5524: reward: 113.00, mean_100:  28.32, epsilon: 0.02, episodes: 250\n",
      "5593: reward:  69.00, mean_100:  28.90, epsilon: 0.02, episodes: 251\n",
      "5664: reward:  71.00, mean_100:  29.28, epsilon: 0.02, episodes: 252\n",
      "5720: reward:  56.00, mean_100:  29.73, epsilon: 0.02, episodes: 253\n",
      "5754: reward:  34.00, mean_100:  29.94, epsilon: 0.02, episodes: 254\n",
      "5846: reward:  92.00, mean_100:  30.71, epsilon: 0.02, episodes: 255\n",
      "5894: reward:  48.00, mean_100:  31.02, epsilon: 0.02, episodes: 256\n",
      "5960: reward:  66.00, mean_100:  31.59, epsilon: 0.02, episodes: 257\n",
      "6027: reward:  67.00, mean_100:  32.16, epsilon: 0.02, episodes: 258\n",
      "6106: reward:  79.00, mean_100:  32.64, epsilon: 0.02, episodes: 259\n",
      "6159: reward:  53.00, mean_100:  33.03, epsilon: 0.02, episodes: 260\n",
      "6216: reward:  57.00, mean_100:  33.50, epsilon: 0.02, episodes: 261\n",
      "6447: reward: 231.00, mean_100:  35.70, epsilon: 0.02, episodes: 262\n",
      "6490: reward:  43.00, mean_100:  35.93, epsilon: 0.02, episodes: 263\n",
      "6543: reward:  53.00, mean_100:  36.20, epsilon: 0.02, episodes: 264\n",
      "6592: reward:  49.00, mean_100:  36.57, epsilon: 0.02, episodes: 265\n",
      "6631: reward:  39.00, mean_100:  36.75, epsilon: 0.02, episodes: 266\n",
      "6695: reward:  64.00, mean_100:  37.21, epsilon: 0.02, episodes: 267\n",
      "6730: reward:  35.00, mean_100:  37.33, epsilon: 0.02, episodes: 268\n",
      "6801: reward:  71.00, mean_100:  37.87, epsilon: 0.02, episodes: 269\n",
      "6859: reward:  58.00, mean_100:  38.25, epsilon: 0.02, episodes: 270\n",
      "6902: reward:  43.00, mean_100:  38.36, epsilon: 0.02, episodes: 271\n",
      "6930: reward:  28.00, mean_100:  38.42, epsilon: 0.02, episodes: 272\n",
      "6985: reward:  55.00, mean_100:  38.61, epsilon: 0.02, episodes: 273\n",
      "7032: reward:  47.00, mean_100:  38.81, epsilon: 0.02, episodes: 274\n",
      "7096: reward:  64.00, mean_100:  38.85, epsilon: 0.02, episodes: 275\n",
      "7145: reward:  49.00, mean_100:  39.08, epsilon: 0.02, episodes: 276\n",
      "7183: reward:  38.00, mean_100:  39.11, epsilon: 0.02, episodes: 277\n",
      "7268: reward:  85.00, mean_100:  39.70, epsilon: 0.02, episodes: 278\n",
      "7300: reward:  32.00, mean_100:  39.72, epsilon: 0.02, episodes: 279\n",
      "7361: reward:  61.00, mean_100:  40.01, epsilon: 0.02, episodes: 280\n",
      "7398: reward:  37.00, mean_100:  40.18, epsilon: 0.02, episodes: 281\n",
      "7429: reward:  31.00, mean_100:  39.68, epsilon: 0.02, episodes: 282\n",
      "7475: reward:  46.00, mean_100:  40.02, epsilon: 0.02, episodes: 283\n",
      "7518: reward:  43.00, mean_100:  40.30, epsilon: 0.02, episodes: 284\n",
      "7557: reward:  39.00, mean_100:  40.52, epsilon: 0.02, episodes: 285\n",
      "7603: reward:  46.00, mean_100:  40.80, epsilon: 0.02, episodes: 286\n",
      "7679: reward:  76.00, mean_100:  41.40, epsilon: 0.02, episodes: 287\n",
      "7735: reward:  56.00, mean_100:  41.80, epsilon: 0.02, episodes: 288\n",
      "7762: reward:  27.00, mean_100:  41.84, epsilon: 0.02, episodes: 289\n",
      "7804: reward:  42.00, mean_100:  42.13, epsilon: 0.02, episodes: 290\n",
      "7842: reward:  38.00, mean_100:  42.36, epsilon: 0.02, episodes: 291\n",
      "7867: reward:  25.00, mean_100:  42.47, epsilon: 0.02, episodes: 292\n",
      "7909: reward:  42.00, mean_100:  42.75, epsilon: 0.02, episodes: 293\n",
      "7947: reward:  38.00, mean_100:  42.97, epsilon: 0.02, episodes: 294\n",
      "8002: reward:  55.00, mean_100:  43.42, epsilon: 0.02, episodes: 295\n",
      "8059: reward:  57.00, mean_100:  43.85, epsilon: 0.02, episodes: 296\n",
      "8119: reward:  60.00, mean_100:  44.32, epsilon: 0.02, episodes: 297\n",
      "8232: reward: 113.00, mean_100:  45.31, epsilon: 0.02, episodes: 298\n",
      "8261: reward:  29.00, mean_100:  45.42, epsilon: 0.02, episodes: 299\n",
      "8298: reward:  37.00, mean_100:  45.67, epsilon: 0.02, episodes: 300\n",
      "8352: reward:  54.00, mean_100:  46.01, epsilon: 0.02, episodes: 301\n",
      "8380: reward:  28.00, mean_100:  46.17, epsilon: 0.02, episodes: 302\n",
      "8447: reward:  67.00, mean_100:  46.70, epsilon: 0.02, episodes: 303\n",
      "8480: reward:  33.00, mean_100:  46.86, epsilon: 0.02, episodes: 304\n",
      "8555: reward:  75.00, mean_100:  47.48, epsilon: 0.02, episodes: 305\n",
      "8600: reward:  45.00, mean_100:  47.79, epsilon: 0.02, episodes: 306\n",
      "8629: reward:  29.00, mean_100:  47.93, epsilon: 0.02, episodes: 307\n",
      "8675: reward:  46.00, mean_100:  48.20, epsilon: 0.02, episodes: 308\n",
      "8704: reward:  29.00, mean_100:  48.30, epsilon: 0.02, episodes: 309\n",
      "8731: reward:  27.00, mean_100:  48.44, epsilon: 0.02, episodes: 310\n",
      "8762: reward:  31.00, mean_100:  48.63, epsilon: 0.02, episodes: 311\n",
      "8789: reward:  27.00, mean_100:  48.69, epsilon: 0.02, episodes: 312\n",
      "8842: reward:  53.00, mean_100:  49.08, epsilon: 0.02, episodes: 313\n",
      "8866: reward:  24.00, mean_100:  49.13, epsilon: 0.02, episodes: 314\n",
      "8917: reward:  51.00, mean_100:  49.41, epsilon: 0.02, episodes: 315\n",
      "8944: reward:  27.00, mean_100:  49.54, epsilon: 0.02, episodes: 316\n",
      "8978: reward:  34.00, mean_100:  49.64, epsilon: 0.02, episodes: 317\n",
      "9003: reward:  25.00, mean_100:  49.12, epsilon: 0.02, episodes: 318\n",
      "9046: reward:  43.00, mean_100:  49.12, epsilon: 0.02, episodes: 319\n",
      "9088: reward:  42.00, mean_100:  49.32, epsilon: 0.02, episodes: 320\n",
      "9113: reward:  25.00, mean_100:  49.30, epsilon: 0.02, episodes: 321\n",
      "9156: reward:  43.00, mean_100:  49.31, epsilon: 0.02, episodes: 322\n",
      "9176: reward:  20.00, mean_100:  49.09, epsilon: 0.02, episodes: 323\n",
      "9261: reward:  85.00, mean_100:  49.43, epsilon: 0.02, episodes: 324\n",
      "9316: reward:  55.00, mean_100:  49.55, epsilon: 0.02, episodes: 325\n",
      "9344: reward:  28.00, mean_100:  49.56, epsilon: 0.02, episodes: 326\n",
      "9394: reward:  50.00, mean_100:  49.60, epsilon: 0.02, episodes: 327\n",
      "9446: reward:  52.00, mean_100:  49.76, epsilon: 0.02, episodes: 328\n",
      "9477: reward:  31.00, mean_100:  49.57, epsilon: 0.02, episodes: 329\n",
      "9504: reward:  27.00, mean_100:  49.37, epsilon: 0.02, episodes: 330\n",
      "9562: reward:  58.00, mean_100:  49.65, epsilon: 0.02, episodes: 331\n",
      "9600: reward:  38.00, mean_100:  49.76, epsilon: 0.02, episodes: 332\n",
      "9666: reward:  66.00, mean_100:  50.01, epsilon: 0.02, episodes: 333\n",
      "9692: reward:  26.00, mean_100:  49.84, epsilon: 0.02, episodes: 334\n",
      "9739: reward:  47.00, mean_100:  49.95, epsilon: 0.02, episodes: 335\n",
      "9773: reward:  34.00, mean_100:  49.83, epsilon: 0.02, episodes: 336\n",
      "9815: reward:  42.00, mean_100:  49.93, epsilon: 0.02, episodes: 337\n",
      "9856: reward:  41.00, mean_100:  49.77, epsilon: 0.02, episodes: 338\n",
      "9892: reward:  36.00, mean_100:  49.75, epsilon: 0.02, episodes: 339\n",
      "9929: reward:  37.00, mean_100:  49.55, epsilon: 0.02, episodes: 340\n",
      "9963: reward:  34.00, mean_100:  49.51, epsilon: 0.02, episodes: 341\n",
      "10010: reward:  47.00, mean_100:  49.36, epsilon: 0.02, episodes: 342\n",
      "10062: reward:  52.00, mean_100:  49.14, epsilon: 0.02, episodes: 343\n",
      "10100: reward:  38.00, mean_100:  49.16, epsilon: 0.02, episodes: 344\n",
      "10134: reward:  34.00, mean_100:  49.13, epsilon: 0.02, episodes: 345\n",
      "10207: reward:  73.00, mean_100:  49.40, epsilon: 0.02, episodes: 346\n",
      "10238: reward:  31.00, mean_100:  49.21, epsilon: 0.02, episodes: 347\n",
      "10266: reward:  28.00, mean_100:  49.07, epsilon: 0.02, episodes: 348\n",
      "10311: reward:  45.00, mean_100:  49.00, epsilon: 0.02, episodes: 349\n",
      "10401: reward:  90.00, mean_100:  48.77, epsilon: 0.02, episodes: 350\n",
      "10443: reward:  42.00, mean_100:  48.50, epsilon: 0.02, episodes: 351\n",
      "10487: reward:  44.00, mean_100:  48.23, epsilon: 0.02, episodes: 352\n",
      "10523: reward:  36.00, mean_100:  48.03, epsilon: 0.02, episodes: 353\n",
      "10551: reward:  28.00, mean_100:  47.97, epsilon: 0.02, episodes: 354\n",
      "10592: reward:  41.00, mean_100:  47.46, epsilon: 0.02, episodes: 355\n",
      "10632: reward:  40.00, mean_100:  47.38, epsilon: 0.02, episodes: 356\n",
      "10653: reward:  21.00, mean_100:  46.93, epsilon: 0.02, episodes: 357\n",
      "10671: reward:  18.00, mean_100:  46.44, epsilon: 0.02, episodes: 358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10693: reward:  22.00, mean_100:  45.87, epsilon: 0.02, episodes: 359\n",
      "10729: reward:  36.00, mean_100:  45.70, epsilon: 0.02, episodes: 360\n",
      "10751: reward:  22.00, mean_100:  45.35, epsilon: 0.02, episodes: 361\n",
      "10777: reward:  26.00, mean_100:  43.30, epsilon: 0.02, episodes: 362\n",
      "10799: reward:  22.00, mean_100:  43.09, epsilon: 0.02, episodes: 363\n",
      "10830: reward:  31.00, mean_100:  42.87, epsilon: 0.02, episodes: 364\n",
      "10847: reward:  17.00, mean_100:  42.55, epsilon: 0.02, episodes: 365\n",
      "10874: reward:  27.00, mean_100:  42.43, epsilon: 0.02, episodes: 366\n",
      "10904: reward:  30.00, mean_100:  42.09, epsilon: 0.02, episodes: 367\n",
      "10919: reward:  15.00, mean_100:  41.89, epsilon: 0.02, episodes: 368\n",
      "10935: reward:  16.00, mean_100:  41.34, epsilon: 0.02, episodes: 369\n",
      "10965: reward:  30.00, mean_100:  41.06, epsilon: 0.02, episodes: 370\n",
      "11002: reward:  37.00, mean_100:  41.00, epsilon: 0.02, episodes: 371\n",
      "11043: reward:  41.00, mean_100:  41.13, epsilon: 0.02, episodes: 372\n",
      "11081: reward:  38.00, mean_100:  40.96, epsilon: 0.02, episodes: 373\n",
      "11106: reward:  25.00, mean_100:  40.74, epsilon: 0.02, episodes: 374\n",
      "11138: reward:  32.00, mean_100:  40.42, epsilon: 0.02, episodes: 375\n",
      "11159: reward:  21.00, mean_100:  40.14, epsilon: 0.02, episodes: 376\n",
      "11187: reward:  28.00, mean_100:  40.04, epsilon: 0.02, episodes: 377\n",
      "11228: reward:  41.00, mean_100:  39.60, epsilon: 0.02, episodes: 378\n",
      "11252: reward:  24.00, mean_100:  39.52, epsilon: 0.02, episodes: 379\n",
      "11275: reward:  23.00, mean_100:  39.14, epsilon: 0.02, episodes: 380\n",
      "11329: reward:  54.00, mean_100:  39.31, epsilon: 0.02, episodes: 381\n",
      "11365: reward:  36.00, mean_100:  39.36, epsilon: 0.02, episodes: 382\n",
      "11394: reward:  29.00, mean_100:  39.19, epsilon: 0.02, episodes: 383\n",
      "11420: reward:  26.00, mean_100:  39.02, epsilon: 0.02, episodes: 384\n",
      "11444: reward:  24.00, mean_100:  38.87, epsilon: 0.02, episodes: 385\n",
      "11475: reward:  31.00, mean_100:  38.72, epsilon: 0.02, episodes: 386\n",
      "11518: reward:  43.00, mean_100:  38.39, epsilon: 0.02, episodes: 387\n",
      "11549: reward:  31.00, mean_100:  38.14, epsilon: 0.02, episodes: 388\n",
      "11605: reward:  56.00, mean_100:  38.43, epsilon: 0.02, episodes: 389\n",
      "11637: reward:  32.00, mean_100:  38.33, epsilon: 0.02, episodes: 390\n",
      "11693: reward:  56.00, mean_100:  38.51, epsilon: 0.02, episodes: 391\n",
      "11719: reward:  26.00, mean_100:  38.52, epsilon: 0.02, episodes: 392\n",
      "11765: reward:  46.00, mean_100:  38.56, epsilon: 0.02, episodes: 393\n",
      "11785: reward:  20.00, mean_100:  38.38, epsilon: 0.02, episodes: 394\n",
      "11804: reward:  19.00, mean_100:  38.02, epsilon: 0.02, episodes: 395\n",
      "11876: reward:  72.00, mean_100:  38.17, epsilon: 0.02, episodes: 396\n",
      "11933: reward:  57.00, mean_100:  38.14, epsilon: 0.02, episodes: 397\n",
      "11957: reward:  24.00, mean_100:  37.25, epsilon: 0.02, episodes: 398\n",
      "11992: reward:  35.00, mean_100:  37.31, epsilon: 0.02, episodes: 399\n",
      "12025: reward:  33.00, mean_100:  37.27, epsilon: 0.02, episodes: 400\n",
      "12054: reward:  29.00, mean_100:  37.02, epsilon: 0.02, episodes: 401\n",
      "12082: reward:  28.00, mean_100:  37.02, epsilon: 0.02, episodes: 402\n",
      "12129: reward:  47.00, mean_100:  36.82, epsilon: 0.02, episodes: 403\n",
      "12159: reward:  30.00, mean_100:  36.79, epsilon: 0.02, episodes: 404\n",
      "12199: reward:  40.00, mean_100:  36.44, epsilon: 0.02, episodes: 405\n",
      "12233: reward:  34.00, mean_100:  36.33, epsilon: 0.02, episodes: 406\n",
      "12259: reward:  26.00, mean_100:  36.30, epsilon: 0.02, episodes: 407\n",
      "12274: reward:  15.00, mean_100:  35.99, epsilon: 0.02, episodes: 408\n",
      "12305: reward:  31.00, mean_100:  36.01, epsilon: 0.02, episodes: 409\n",
      "12331: reward:  26.00, mean_100:  36.00, epsilon: 0.02, episodes: 410\n",
      "12361: reward:  30.00, mean_100:  35.99, epsilon: 0.02, episodes: 411\n",
      "12384: reward:  23.00, mean_100:  35.95, epsilon: 0.02, episodes: 412\n",
      "12417: reward:  33.00, mean_100:  35.75, epsilon: 0.02, episodes: 413\n",
      "12457: reward:  40.00, mean_100:  35.91, epsilon: 0.02, episodes: 414\n",
      "12480: reward:  23.00, mean_100:  35.63, epsilon: 0.02, episodes: 415\n",
      "12497: reward:  17.00, mean_100:  35.53, epsilon: 0.02, episodes: 416\n",
      "12541: reward:  44.00, mean_100:  35.63, epsilon: 0.02, episodes: 417\n",
      "12580: reward:  39.00, mean_100:  35.77, epsilon: 0.02, episodes: 418\n",
      "12609: reward:  29.00, mean_100:  35.63, epsilon: 0.02, episodes: 419\n",
      "12655: reward:  46.00, mean_100:  35.67, epsilon: 0.02, episodes: 420\n",
      "12678: reward:  23.00, mean_100:  35.65, epsilon: 0.02, episodes: 421\n",
      "12709: reward:  31.00, mean_100:  35.53, epsilon: 0.02, episodes: 422\n",
      "12758: reward:  49.00, mean_100:  35.82, epsilon: 0.02, episodes: 423\n",
      "12785: reward:  27.00, mean_100:  35.24, epsilon: 0.02, episodes: 424\n",
      "12825: reward:  40.00, mean_100:  35.09, epsilon: 0.02, episodes: 425\n",
      "12859: reward:  34.00, mean_100:  35.15, epsilon: 0.02, episodes: 426\n",
      "12880: reward:  21.00, mean_100:  34.86, epsilon: 0.02, episodes: 427\n",
      "12915: reward:  35.00, mean_100:  34.69, epsilon: 0.02, episodes: 428\n",
      "12933: reward:  18.00, mean_100:  34.56, epsilon: 0.02, episodes: 429\n",
      "13004: reward:  71.00, mean_100:  35.00, epsilon: 0.02, episodes: 430\n",
      "13038: reward:  34.00, mean_100:  34.76, epsilon: 0.02, episodes: 431\n",
      "13071: reward:  33.00, mean_100:  34.71, epsilon: 0.02, episodes: 432\n",
      "13087: reward:  16.00, mean_100:  34.21, epsilon: 0.02, episodes: 433\n",
      "13120: reward:  33.00, mean_100:  34.28, epsilon: 0.02, episodes: 434\n",
      "13143: reward:  23.00, mean_100:  34.04, epsilon: 0.02, episodes: 435\n",
      "13170: reward:  27.00, mean_100:  33.97, epsilon: 0.02, episodes: 436\n",
      "13225: reward:  55.00, mean_100:  34.10, epsilon: 0.02, episodes: 437\n",
      "13248: reward:  23.00, mean_100:  33.92, epsilon: 0.02, episodes: 438\n",
      "13258: reward:  10.00, mean_100:  33.66, epsilon: 0.02, episodes: 439\n",
      "13297: reward:  39.00, mean_100:  33.68, epsilon: 0.02, episodes: 440\n",
      "13336: reward:  39.00, mean_100:  33.73, epsilon: 0.02, episodes: 441\n",
      "13362: reward:  26.00, mean_100:  33.52, epsilon: 0.02, episodes: 442\n",
      "13383: reward:  21.00, mean_100:  33.21, epsilon: 0.02, episodes: 443\n",
      "13409: reward:  26.00, mean_100:  33.09, epsilon: 0.02, episodes: 444\n",
      "13439: reward:  30.00, mean_100:  33.05, epsilon: 0.02, episodes: 445\n",
      "13487: reward:  48.00, mean_100:  32.80, epsilon: 0.02, episodes: 446\n",
      "13545: reward:  58.00, mean_100:  33.07, epsilon: 0.02, episodes: 447\n",
      "13631: reward:  86.00, mean_100:  33.65, epsilon: 0.02, episodes: 448\n",
      "13673: reward:  42.00, mean_100:  33.62, epsilon: 0.02, episodes: 449\n",
      "13689: reward:  16.00, mean_100:  32.88, epsilon: 0.02, episodes: 450\n",
      "13725: reward:  36.00, mean_100:  32.82, epsilon: 0.02, episodes: 451\n",
      "13758: reward:  33.00, mean_100:  32.71, epsilon: 0.02, episodes: 452\n",
      "13841: reward:  83.00, mean_100:  33.18, epsilon: 0.02, episodes: 453\n",
      "13864: reward:  23.00, mean_100:  33.13, epsilon: 0.02, episodes: 454\n",
      "13895: reward:  31.00, mean_100:  33.03, epsilon: 0.02, episodes: 455\n",
      "13926: reward:  31.00, mean_100:  32.94, epsilon: 0.02, episodes: 456\n",
      "13946: reward:  20.00, mean_100:  32.93, epsilon: 0.02, episodes: 457\n",
      "14003: reward:  57.00, mean_100:  33.32, epsilon: 0.02, episodes: 458\n",
      "14018: reward:  15.00, mean_100:  33.25, epsilon: 0.02, episodes: 459\n",
      "14042: reward:  24.00, mean_100:  33.13, epsilon: 0.02, episodes: 460\n",
      "14080: reward:  38.00, mean_100:  33.29, epsilon: 0.02, episodes: 461\n",
      "14131: reward:  51.00, mean_100:  33.54, epsilon: 0.02, episodes: 462\n",
      "14154: reward:  23.00, mean_100:  33.55, epsilon: 0.02, episodes: 463\n",
      "14173: reward:  19.00, mean_100:  33.43, epsilon: 0.02, episodes: 464\n",
      "14219: reward:  46.00, mean_100:  33.72, epsilon: 0.02, episodes: 465\n",
      "14283: reward:  64.00, mean_100:  34.09, epsilon: 0.02, episodes: 466\n",
      "14339: reward:  56.00, mean_100:  34.35, epsilon: 0.02, episodes: 467\n",
      "14374: reward:  35.00, mean_100:  34.55, epsilon: 0.02, episodes: 468\n",
      "14416: reward:  42.00, mean_100:  34.81, epsilon: 0.02, episodes: 469\n",
      "14441: reward:  25.00, mean_100:  34.76, epsilon: 0.02, episodes: 470\n",
      "14484: reward:  43.00, mean_100:  34.82, epsilon: 0.02, episodes: 471\n",
      "14519: reward:  35.00, mean_100:  34.76, epsilon: 0.02, episodes: 472\n",
      "14555: reward:  36.00, mean_100:  34.74, epsilon: 0.02, episodes: 473\n",
      "14580: reward:  25.00, mean_100:  34.74, epsilon: 0.02, episodes: 474\n",
      "14590: reward:  10.00, mean_100:  34.52, epsilon: 0.02, episodes: 475\n",
      "14613: reward:  23.00, mean_100:  34.54, epsilon: 0.02, episodes: 476\n",
      "14649: reward:  36.00, mean_100:  34.62, epsilon: 0.02, episodes: 477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14673: reward:  24.00, mean_100:  34.45, epsilon: 0.02, episodes: 478\n",
      "14701: reward:  28.00, mean_100:  34.49, epsilon: 0.02, episodes: 479\n",
      "14735: reward:  34.00, mean_100:  34.60, epsilon: 0.02, episodes: 480\n",
      "14758: reward:  23.00, mean_100:  34.29, epsilon: 0.02, episodes: 481\n",
      "14811: reward:  53.00, mean_100:  34.46, epsilon: 0.02, episodes: 482\n",
      "14865: reward:  54.00, mean_100:  34.71, epsilon: 0.02, episodes: 483\n",
      "14889: reward:  24.00, mean_100:  34.69, epsilon: 0.02, episodes: 484\n",
      "14950: reward:  61.00, mean_100:  35.06, epsilon: 0.02, episodes: 485\n",
      "14990: reward:  40.00, mean_100:  35.15, epsilon: 0.02, episodes: 486\n",
      "15036: reward:  46.00, mean_100:  35.18, epsilon: 0.02, episodes: 487\n",
      "15056: reward:  20.00, mean_100:  35.07, epsilon: 0.02, episodes: 488\n",
      "15072: reward:  16.00, mean_100:  34.67, epsilon: 0.02, episodes: 489\n",
      "15153: reward:  81.00, mean_100:  35.16, epsilon: 0.02, episodes: 490\n",
      "15186: reward:  33.00, mean_100:  34.93, epsilon: 0.02, episodes: 491\n",
      "15219: reward:  33.00, mean_100:  35.00, epsilon: 0.02, episodes: 492\n",
      "15261: reward:  42.00, mean_100:  34.96, epsilon: 0.02, episodes: 493\n",
      "15299: reward:  38.00, mean_100:  35.14, epsilon: 0.02, episodes: 494\n",
      "15314: reward:  15.00, mean_100:  35.10, epsilon: 0.02, episodes: 495\n",
      "15339: reward:  25.00, mean_100:  34.63, epsilon: 0.02, episodes: 496\n",
      "15372: reward:  33.00, mean_100:  34.39, epsilon: 0.02, episodes: 497\n",
      "15398: reward:  26.00, mean_100:  34.41, epsilon: 0.02, episodes: 498\n",
      "15421: reward:  23.00, mean_100:  34.29, epsilon: 0.02, episodes: 499\n",
      "15439: reward:  18.00, mean_100:  34.14, epsilon: 0.02, episodes: 500\n",
      "15476: reward:  37.00, mean_100:  34.22, epsilon: 0.02, episodes: 501\n",
      "15504: reward:  28.00, mean_100:  34.22, epsilon: 0.02, episodes: 502\n",
      "15552: reward:  48.00, mean_100:  34.23, epsilon: 0.02, episodes: 503\n",
      "15583: reward:  31.00, mean_100:  34.24, epsilon: 0.02, episodes: 504\n",
      "15649: reward:  66.00, mean_100:  34.50, epsilon: 0.02, episodes: 505\n",
      "15698: reward:  49.00, mean_100:  34.65, epsilon: 0.02, episodes: 506\n",
      "15717: reward:  19.00, mean_100:  34.58, epsilon: 0.02, episodes: 507\n",
      "15757: reward:  40.00, mean_100:  34.83, epsilon: 0.02, episodes: 508\n",
      "15771: reward:  14.00, mean_100:  34.66, epsilon: 0.02, episodes: 509\n",
      "15799: reward:  28.00, mean_100:  34.68, epsilon: 0.02, episodes: 510\n",
      "15836: reward:  37.00, mean_100:  34.75, epsilon: 0.02, episodes: 511\n",
      "15909: reward:  73.00, mean_100:  35.25, epsilon: 0.02, episodes: 512\n",
      "15955: reward:  46.00, mean_100:  35.38, epsilon: 0.02, episodes: 513\n",
      "15972: reward:  17.00, mean_100:  35.15, epsilon: 0.02, episodes: 514\n",
      "16010: reward:  38.00, mean_100:  35.30, epsilon: 0.02, episodes: 515\n",
      "16061: reward:  51.00, mean_100:  35.64, epsilon: 0.02, episodes: 516\n",
      "16146: reward:  85.00, mean_100:  36.05, epsilon: 0.02, episodes: 517\n",
      "16202: reward:  56.00, mean_100:  36.22, epsilon: 0.02, episodes: 518\n",
      "16233: reward:  31.00, mean_100:  36.24, epsilon: 0.02, episodes: 519\n",
      "16261: reward:  28.00, mean_100:  36.06, epsilon: 0.02, episodes: 520\n",
      "16307: reward:  46.00, mean_100:  36.29, epsilon: 0.02, episodes: 521\n",
      "16336: reward:  29.00, mean_100:  36.27, epsilon: 0.02, episodes: 522\n",
      "16401: reward:  65.00, mean_100:  36.43, epsilon: 0.02, episodes: 523\n",
      "16448: reward:  47.00, mean_100:  36.63, epsilon: 0.02, episodes: 524\n",
      "16483: reward:  35.00, mean_100:  36.58, epsilon: 0.02, episodes: 525\n",
      "16504: reward:  21.00, mean_100:  36.45, epsilon: 0.02, episodes: 526\n",
      "16540: reward:  36.00, mean_100:  36.60, epsilon: 0.02, episodes: 527\n",
      "16551: reward:  11.00, mean_100:  36.36, epsilon: 0.02, episodes: 528\n",
      "16563: reward:  12.00, mean_100:  36.30, epsilon: 0.02, episodes: 529\n",
      "16593: reward:  30.00, mean_100:  35.89, epsilon: 0.02, episodes: 530\n",
      "16623: reward:  30.00, mean_100:  35.85, epsilon: 0.02, episodes: 531\n",
      "16632: reward:   9.00, mean_100:  35.61, epsilon: 0.02, episodes: 532\n",
      "16650: reward:  18.00, mean_100:  35.63, epsilon: 0.02, episodes: 533\n",
      "16698: reward:  48.00, mean_100:  35.78, epsilon: 0.02, episodes: 534\n",
      "16758: reward:  60.00, mean_100:  36.15, epsilon: 0.02, episodes: 535\n",
      "16816: reward:  58.00, mean_100:  36.46, epsilon: 0.02, episodes: 536\n",
      "16842: reward:  26.00, mean_100:  36.17, epsilon: 0.02, episodes: 537\n",
      "16923: reward:  81.00, mean_100:  36.75, epsilon: 0.02, episodes: 538\n",
      "16931: reward:   8.00, mean_100:  36.73, epsilon: 0.02, episodes: 539\n",
      "16982: reward:  51.00, mean_100:  36.85, epsilon: 0.02, episodes: 540\n",
      "17020: reward:  38.00, mean_100:  36.84, epsilon: 0.02, episodes: 541\n",
      "17039: reward:  19.00, mean_100:  36.77, epsilon: 0.02, episodes: 542\n",
      "17079: reward:  40.00, mean_100:  36.96, epsilon: 0.02, episodes: 543\n",
      "17094: reward:  15.00, mean_100:  36.85, epsilon: 0.02, episodes: 544\n",
      "17115: reward:  21.00, mean_100:  36.76, epsilon: 0.02, episodes: 545\n",
      "17133: reward:  18.00, mean_100:  36.46, epsilon: 0.02, episodes: 546\n",
      "17148: reward:  15.00, mean_100:  36.03, epsilon: 0.02, episodes: 547\n",
      "17192: reward:  44.00, mean_100:  35.61, epsilon: 0.02, episodes: 548\n",
      "17216: reward:  24.00, mean_100:  35.43, epsilon: 0.02, episodes: 549\n",
      "17262: reward:  46.00, mean_100:  35.73, epsilon: 0.02, episodes: 550\n",
      "17320: reward:  58.00, mean_100:  35.95, epsilon: 0.02, episodes: 551\n",
      "17365: reward:  45.00, mean_100:  36.07, epsilon: 0.02, episodes: 552\n",
      "17449: reward:  84.00, mean_100:  36.08, epsilon: 0.02, episodes: 553\n",
      "17516: reward:  67.00, mean_100:  36.52, epsilon: 0.02, episodes: 554\n",
      "17589: reward:  73.00, mean_100:  36.94, epsilon: 0.02, episodes: 555\n",
      "17643: reward:  54.00, mean_100:  37.17, epsilon: 0.02, episodes: 556\n",
      "17705: reward:  62.00, mean_100:  37.59, epsilon: 0.02, episodes: 557\n",
      "17735: reward:  30.00, mean_100:  37.32, epsilon: 0.02, episodes: 558\n",
      "17811: reward:  76.00, mean_100:  37.93, epsilon: 0.02, episodes: 559\n",
      "17881: reward:  70.00, mean_100:  38.39, epsilon: 0.02, episodes: 560\n",
      "17922: reward:  41.00, mean_100:  38.42, epsilon: 0.02, episodes: 561\n",
      "17950: reward:  28.00, mean_100:  38.19, epsilon: 0.02, episodes: 562\n",
      "18011: reward:  61.00, mean_100:  38.57, epsilon: 0.02, episodes: 563\n",
      "18030: reward:  19.00, mean_100:  38.57, epsilon: 0.02, episodes: 564\n",
      "18097: reward:  67.00, mean_100:  38.78, epsilon: 0.02, episodes: 565\n",
      "18135: reward:  38.00, mean_100:  38.52, epsilon: 0.02, episodes: 566\n",
      "18181: reward:  46.00, mean_100:  38.42, epsilon: 0.02, episodes: 567\n",
      "18232: reward:  51.00, mean_100:  38.58, epsilon: 0.02, episodes: 568\n",
      "18298: reward:  66.00, mean_100:  38.82, epsilon: 0.02, episodes: 569\n",
      "18364: reward:  66.00, mean_100:  39.23, epsilon: 0.02, episodes: 570\n",
      "18401: reward:  37.00, mean_100:  39.17, epsilon: 0.02, episodes: 571\n",
      "18476: reward:  75.00, mean_100:  39.57, epsilon: 0.02, episodes: 572\n",
      "18584: reward: 108.00, mean_100:  40.29, epsilon: 0.02, episodes: 573\n",
      "18630: reward:  46.00, mean_100:  40.50, epsilon: 0.02, episodes: 574\n",
      "18643: reward:  13.00, mean_100:  40.53, epsilon: 0.02, episodes: 575\n",
      "18661: reward:  18.00, mean_100:  40.48, epsilon: 0.02, episodes: 576\n",
      "18731: reward:  70.00, mean_100:  40.82, epsilon: 0.02, episodes: 577\n",
      "18747: reward:  16.00, mean_100:  40.74, epsilon: 0.02, episodes: 578\n",
      "18773: reward:  26.00, mean_100:  40.72, epsilon: 0.02, episodes: 579\n",
      "18824: reward:  51.00, mean_100:  40.89, epsilon: 0.02, episodes: 580\n",
      "18867: reward:  43.00, mean_100:  41.09, epsilon: 0.02, episodes: 581\n",
      "18939: reward:  72.00, mean_100:  41.28, epsilon: 0.02, episodes: 582\n",
      "19007: reward:  68.00, mean_100:  41.42, epsilon: 0.02, episodes: 583\n",
      "19061: reward:  54.00, mean_100:  41.72, epsilon: 0.02, episodes: 584\n",
      "19100: reward:  39.00, mean_100:  41.50, epsilon: 0.02, episodes: 585\n",
      "19156: reward:  56.00, mean_100:  41.66, epsilon: 0.02, episodes: 586\n",
      "19194: reward:  38.00, mean_100:  41.58, epsilon: 0.02, episodes: 587\n",
      "19257: reward:  63.00, mean_100:  42.01, epsilon: 0.02, episodes: 588\n",
      "19267: reward:  10.00, mean_100:  41.95, epsilon: 0.02, episodes: 589\n",
      "19315: reward:  48.00, mean_100:  41.62, epsilon: 0.02, episodes: 590\n",
      "19348: reward:  33.00, mean_100:  41.62, epsilon: 0.02, episodes: 591\n",
      "19396: reward:  48.00, mean_100:  41.77, epsilon: 0.02, episodes: 592\n",
      "19462: reward:  66.00, mean_100:  42.01, epsilon: 0.02, episodes: 593\n",
      "19525: reward:  63.00, mean_100:  42.26, epsilon: 0.02, episodes: 594\n",
      "19696: reward: 171.00, mean_100:  43.82, epsilon: 0.02, episodes: 595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19774: reward:  78.00, mean_100:  44.35, epsilon: 0.02, episodes: 596\n",
      "19807: reward:  33.00, mean_100:  44.35, epsilon: 0.02, episodes: 597\n",
      "19863: reward:  56.00, mean_100:  44.65, epsilon: 0.02, episodes: 598\n",
      "19890: reward:  27.00, mean_100:  44.69, epsilon: 0.02, episodes: 599\n",
      "19993: reward: 103.00, mean_100:  45.54, epsilon: 0.02, episodes: 600\n",
      "20063: reward:  70.00, mean_100:  45.87, epsilon: 0.02, episodes: 601\n",
      "20167: reward: 104.00, mean_100:  46.63, epsilon: 0.02, episodes: 602\n",
      "20234: reward:  67.00, mean_100:  46.82, epsilon: 0.02, episodes: 603\n",
      "20307: reward:  73.00, mean_100:  47.24, epsilon: 0.02, episodes: 604\n",
      "20396: reward:  89.00, mean_100:  47.47, epsilon: 0.02, episodes: 605\n",
      "20461: reward:  65.00, mean_100:  47.63, epsilon: 0.02, episodes: 606\n",
      "20497: reward:  36.00, mean_100:  47.80, epsilon: 0.02, episodes: 607\n",
      "20553: reward:  56.00, mean_100:  47.96, epsilon: 0.02, episodes: 608\n",
      "20599: reward:  46.00, mean_100:  48.28, epsilon: 0.02, episodes: 609\n",
      "20647: reward:  48.00, mean_100:  48.48, epsilon: 0.02, episodes: 610\n",
      "20699: reward:  52.00, mean_100:  48.63, epsilon: 0.02, episodes: 611\n",
      "20827: reward: 128.00, mean_100:  49.18, epsilon: 0.02, episodes: 612\n",
      "20896: reward:  69.00, mean_100:  49.41, epsilon: 0.02, episodes: 613\n",
      "20931: reward:  35.00, mean_100:  49.59, epsilon: 0.02, episodes: 614\n",
      "20993: reward:  62.00, mean_100:  49.83, epsilon: 0.02, episodes: 615\n",
      "21128: reward: 135.00, mean_100:  50.67, epsilon: 0.02, episodes: 616\n",
      "21180: reward:  52.00, mean_100:  50.34, epsilon: 0.02, episodes: 617\n",
      "21279: reward:  99.00, mean_100:  50.77, epsilon: 0.02, episodes: 618\n",
      "21323: reward:  44.00, mean_100:  50.90, epsilon: 0.02, episodes: 619\n",
      "21400: reward:  77.00, mean_100:  51.39, epsilon: 0.02, episodes: 620\n",
      "21484: reward:  84.00, mean_100:  51.77, epsilon: 0.02, episodes: 621\n",
      "21513: reward:  29.00, mean_100:  51.77, epsilon: 0.02, episodes: 622\n",
      "21523: reward:  10.00, mean_100:  51.22, epsilon: 0.02, episodes: 623\n",
      "21574: reward:  51.00, mean_100:  51.26, epsilon: 0.02, episodes: 624\n",
      "21593: reward:  19.00, mean_100:  51.10, epsilon: 0.02, episodes: 625\n",
      "21607: reward:  14.00, mean_100:  51.03, epsilon: 0.02, episodes: 626\n",
      "21721: reward: 114.00, mean_100:  51.81, epsilon: 0.02, episodes: 627\n",
      "21816: reward:  95.00, mean_100:  52.65, epsilon: 0.02, episodes: 628\n",
      "21829: reward:  13.00, mean_100:  52.66, epsilon: 0.02, episodes: 629\n",
      "21906: reward:  77.00, mean_100:  53.13, epsilon: 0.02, episodes: 630\n",
      "21946: reward:  40.00, mean_100:  53.23, epsilon: 0.02, episodes: 631\n",
      "22022: reward:  76.00, mean_100:  53.90, epsilon: 0.02, episodes: 632\n",
      "22143: reward: 121.00, mean_100:  54.93, epsilon: 0.02, episodes: 633\n",
      "22223: reward:  80.00, mean_100:  55.25, epsilon: 0.02, episodes: 634\n",
      "22416: reward: 193.00, mean_100:  56.58, epsilon: 0.02, episodes: 635\n",
      "22511: reward:  95.00, mean_100:  56.95, epsilon: 0.02, episodes: 636\n",
      "22523: reward:  12.00, mean_100:  56.81, epsilon: 0.02, episodes: 637\n",
      "22655: reward: 132.00, mean_100:  57.32, epsilon: 0.02, episodes: 638\n",
      "22711: reward:  56.00, mean_100:  57.80, epsilon: 0.02, episodes: 639\n",
      "22771: reward:  60.00, mean_100:  57.89, epsilon: 0.02, episodes: 640\n",
      "22946: reward: 175.00, mean_100:  59.26, epsilon: 0.02, episodes: 641\n",
      "23093: reward: 147.00, mean_100:  60.54, epsilon: 0.02, episodes: 642\n",
      "23243: reward: 150.00, mean_100:  61.64, epsilon: 0.02, episodes: 643\n",
      "23328: reward:  85.00, mean_100:  62.34, epsilon: 0.02, episodes: 644\n",
      "23436: reward: 108.00, mean_100:  63.21, epsilon: 0.02, episodes: 645\n",
      "23650: reward: 214.00, mean_100:  65.17, epsilon: 0.02, episodes: 646\n",
      "23830: reward: 180.00, mean_100:  66.82, epsilon: 0.02, episodes: 647\n",
      "24021: reward: 191.00, mean_100:  68.29, epsilon: 0.02, episodes: 648\n",
      "24139: reward: 118.00, mean_100:  69.23, epsilon: 0.02, episodes: 649\n",
      "24274: reward: 135.00, mean_100:  70.12, epsilon: 0.02, episodes: 650\n",
      "24404: reward: 130.00, mean_100:  70.84, epsilon: 0.02, episodes: 651\n",
      "24560: reward: 156.00, mean_100:  71.95, epsilon: 0.02, episodes: 652\n",
      "24678: reward: 118.00, mean_100:  72.29, epsilon: 0.02, episodes: 653\n",
      "24816: reward: 138.00, mean_100:  73.00, epsilon: 0.02, episodes: 654\n",
      "25016: reward: 200.00, mean_100:  74.27, epsilon: 0.02, episodes: 655\n",
      "25166: reward: 150.00, mean_100:  75.23, epsilon: 0.02, episodes: 656\n",
      "25303: reward: 137.00, mean_100:  75.98, epsilon: 0.02, episodes: 657\n",
      "25448: reward: 145.00, mean_100:  77.13, epsilon: 0.02, episodes: 658\n",
      "25650: reward: 202.00, mean_100:  78.39, epsilon: 0.02, episodes: 659\n",
      "25796: reward: 146.00, mean_100:  79.15, epsilon: 0.02, episodes: 660\n",
      "25904: reward: 108.00, mean_100:  79.82, epsilon: 0.02, episodes: 661\n",
      "26022: reward: 118.00, mean_100:  80.72, epsilon: 0.02, episodes: 662\n",
      "26370: reward: 348.00, mean_100:  83.59, epsilon: 0.02, episodes: 663\n",
      "26550: reward: 180.00, mean_100:  85.20, epsilon: 0.02, episodes: 664\n",
      "26733: reward: 183.00, mean_100:  86.36, epsilon: 0.02, episodes: 665\n",
      "26943: reward: 210.00, mean_100:  88.08, epsilon: 0.02, episodes: 666\n",
      "27125: reward: 182.00, mean_100:  89.44, epsilon: 0.02, episodes: 667\n",
      "27297: reward: 172.00, mean_100:  90.65, epsilon: 0.02, episodes: 668\n",
      "27492: reward: 195.00, mean_100:  91.94, epsilon: 0.02, episodes: 669\n",
      "27695: reward: 203.00, mean_100:  93.31, epsilon: 0.02, episodes: 670\n",
      "27853: reward: 158.00, mean_100:  94.52, epsilon: 0.02, episodes: 671\n",
      "28033: reward: 180.00, mean_100:  95.57, epsilon: 0.02, episodes: 672\n",
      "28259: reward: 226.00, mean_100:  96.75, epsilon: 0.02, episodes: 673\n",
      "28475: reward: 216.00, mean_100:  98.45, epsilon: 0.02, episodes: 674\n",
      "28643: reward: 168.00, mean_100: 100.00, epsilon: 0.02, episodes: 675\n",
      "28837: reward: 194.00, mean_100: 101.76, epsilon: 0.02, episodes: 676\n",
      "29022: reward: 185.00, mean_100: 102.91, epsilon: 0.02, episodes: 677\n",
      "29285: reward: 263.00, mean_100: 105.38, epsilon: 0.02, episodes: 678\n",
      "29500: reward: 215.00, mean_100: 107.27, epsilon: 0.02, episodes: 679\n",
      "29665: reward: 165.00, mean_100: 108.41, epsilon: 0.02, episodes: 680\n",
      "29881: reward: 216.00, mean_100: 110.14, epsilon: 0.02, episodes: 681\n",
      "30135: reward: 254.00, mean_100: 111.96, epsilon: 0.02, episodes: 682\n",
      "30371: reward: 236.00, mean_100: 113.64, epsilon: 0.02, episodes: 683\n",
      "30607: reward: 236.00, mean_100: 115.46, epsilon: 0.02, episodes: 684\n",
      "30796: reward: 189.00, mean_100: 116.96, epsilon: 0.02, episodes: 685\n",
      "31048: reward: 252.00, mean_100: 118.92, epsilon: 0.02, episodes: 686\n",
      "31548: reward: 500.00, mean_100: 123.54, epsilon: 0.02, episodes: 687\n",
      "31804: reward: 256.00, mean_100: 125.47, epsilon: 0.02, episodes: 688\n",
      "32040: reward: 236.00, mean_100: 127.73, epsilon: 0.02, episodes: 689\n",
      "32254: reward: 214.00, mean_100: 129.39, epsilon: 0.02, episodes: 690\n",
      "32630: reward: 376.00, mean_100: 132.82, epsilon: 0.02, episodes: 691\n",
      "32850: reward: 220.00, mean_100: 134.54, epsilon: 0.02, episodes: 692\n",
      "33194: reward: 344.00, mean_100: 137.32, epsilon: 0.02, episodes: 693\n",
      "33415: reward: 221.00, mean_100: 138.90, epsilon: 0.02, episodes: 694\n",
      "33770: reward: 355.00, mean_100: 140.74, epsilon: 0.02, episodes: 695\n",
      "34005: reward: 235.00, mean_100: 142.31, epsilon: 0.02, episodes: 696\n",
      "34186: reward: 181.00, mean_100: 143.79, epsilon: 0.02, episodes: 697\n",
      "34422: reward: 236.00, mean_100: 145.59, epsilon: 0.02, episodes: 698\n",
      "34825: reward: 403.00, mean_100: 149.35, epsilon: 0.02, episodes: 699\n",
      "35100: reward: 275.00, mean_100: 151.07, epsilon: 0.02, episodes: 700\n",
      "35386: reward: 286.00, mean_100: 153.23, epsilon: 0.02, episodes: 701\n",
      "35702: reward: 316.00, mean_100: 155.35, epsilon: 0.02, episodes: 702\n",
      "36083: reward: 381.00, mean_100: 158.49, epsilon: 0.02, episodes: 703\n",
      "36300: reward: 217.00, mean_100: 159.93, epsilon: 0.02, episodes: 704\n",
      "36657: reward: 357.00, mean_100: 162.61, epsilon: 0.02, episodes: 705\n",
      "36962: reward: 305.00, mean_100: 165.01, epsilon: 0.02, episodes: 706\n",
      "37313: reward: 351.00, mean_100: 168.16, epsilon: 0.02, episodes: 707\n",
      "37651: reward: 338.00, mean_100: 170.98, epsilon: 0.02, episodes: 708\n",
      "37909: reward: 258.00, mean_100: 173.10, epsilon: 0.02, episodes: 709\n",
      "38193: reward: 284.00, mean_100: 175.46, epsilon: 0.02, episodes: 710\n",
      "38449: reward: 256.00, mean_100: 177.50, epsilon: 0.02, episodes: 711\n",
      "38722: reward: 273.00, mean_100: 178.95, epsilon: 0.02, episodes: 712\n",
      "38988: reward: 266.00, mean_100: 180.92, epsilon: 0.02, episodes: 713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39242: reward: 254.00, mean_100: 183.11, epsilon: 0.02, episodes: 714\n",
      "39550: reward: 308.00, mean_100: 185.57, epsilon: 0.02, episodes: 715\n",
      "39896: reward: 346.00, mean_100: 187.68, epsilon: 0.02, episodes: 716\n",
      "40221: reward: 325.00, mean_100: 190.41, epsilon: 0.02, episodes: 717\n",
      "40529: reward: 308.00, mean_100: 192.50, epsilon: 0.02, episodes: 718\n",
      "40874: reward: 345.00, mean_100: 195.51, epsilon: 0.02, episodes: 719\n",
      "Solved in 40874 steps and 719 episodes!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    writer = SummaryWriter(comment=\"-cartpole-dqn\")\n",
    "\n",
    "    net = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "    print(net)\n",
    "    \n",
    "    '''we again make use of the ptan library'''\n",
    "    \n",
    "    \n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=EPSILON_START)\n",
    "    agent = ptan.agent.DQNAgent(net, selector, preprocessor=ptan.agent.float32_preprocessor)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)\n",
    "    replay_buffer = ptan.experience.ExperienceReplayBuffer(exp_source, REPLAY_BUFFER)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    mse_loss = nn.MSELoss()\n",
    "\n",
    "    total_rewards = []\n",
    "    step_idx = 0\n",
    "    done_episodes = 0\n",
    "\n",
    "    while True:\n",
    "        step_idx += 1\n",
    "        selector.epsilon = max(EPSILON_STOP, EPSILON_START - step_idx / EPSILON_STEPS)\n",
    "        replay_buffer.populate(1)\n",
    "\n",
    "        if len(replay_buffer) < BATCH_SIZE:\n",
    "            continue\n",
    "\n",
    "        # sample batch\n",
    "        batch = replay_buffer.sample(BATCH_SIZE)\n",
    "        batch_states = [exp.state for exp in batch]\n",
    "        batch_actions = [exp.action for exp in batch]\n",
    "        batch_targets = [calc_target(net, exp.reward, exp.last_state)\n",
    "                         for exp in batch]\n",
    "        # train\n",
    "        optimizer.zero_grad()\n",
    "        states_v = torch.FloatTensor(batch_states)\n",
    "        net_q_v = net(states_v)\n",
    "        target_q = net_q_v.data.numpy().copy()\n",
    "        target_q[range(BATCH_SIZE), batch_actions] = batch_targets\n",
    "        target_q_v = torch.tensor(target_q)\n",
    "        loss_v = mse_loss(net_q_v, target_q_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # handle new rewards\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "            done_episodes += 1\n",
    "            reward = new_rewards[0]\n",
    "            total_rewards.append(reward)\n",
    "            mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "            print(\"%d: reward: %6.2f, mean_100: %6.2f, epsilon: %.2f, episodes: %d\" % (\n",
    "                step_idx, reward, mean_rewards, selector.epsilon, done_episodes))\n",
    "            writer.add_scalar(\"reward\", reward, step_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "            writer.add_scalar(\"epsilon\", selector.epsilon, step_idx)\n",
    "            writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "            if mean_rewards > 195:\n",
    "                print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n",
    "                break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
