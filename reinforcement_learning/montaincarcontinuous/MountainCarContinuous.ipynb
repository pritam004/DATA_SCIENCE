{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving OpenAI Gym's Mountain Car Continuous Control task using Deep Deterministic Policy Gradients  \n",
    "\n",
    "This notebook uses a modified version of [Udacity's DDPG model](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum) to solve OpenAI Gym's [MountainCarContinuous-v0](https://github.com/openai/gym/wiki/MountainCarContinuous-v0) continuous control problem using [Deep Deterministic Policy Gradients (DDPG)](https://arxiv.org/abs/1509.02971) as part of the [Machine Learning Engineer Nanodegree](https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009t) quadcopter project. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the classes are borrowed from Udacity Project and github repos\n",
    "\n",
    "\n",
    "I have worked to tune the hyperparams...and understand the working of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous action space: (-1.000 to 1.000)\n",
      "Reward range: (-inf, inf)\n",
      "Observation range, dimension 0: (-1.200 to 0.600)\n",
      "Observation range, dimension 1: (-0.070 to 0.070)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "print('Continuous action space: (%.3f to %.3f)'%(env.action_space.low, env.action_space.high))\n",
    "print('Reward range: %s'%(str(env.reward_range)))\n",
    "for i in range(len(env.observation_space.low)):\n",
    "    print('Observation range, dimension %i: (%.3f to %.3f)'%\n",
    "          (i,env.observation_space.low[i], env.observation_space.high[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import sys\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import layers, models, optimizers, regularizers\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "lel=[]\n",
    "\n",
    "class DDPG():\n",
    "    \"\"\"Reinforcement Learning agent that learns using DDPG.\"\"\"\n",
    "    def __init__(self, env, train_during_episode=True,\n",
    "                 discount_factor=.999,\n",
    "                 tau_actor=.2, tau_critic=.2,\n",
    "                 lr_actor=.0001, lr_critic=.005,\n",
    "                 bn_momentum_actor=.9, bn_momentum_critic=.9,\n",
    "                 ou_mu=0, ou_theta=.1, ou_sigma=1,\n",
    "                 activation_fn_actor='sigmoid',\n",
    "                 replay_buffer_size=10000, replay_batch_size=64,\n",
    "                 l2_reg_actor=.01, l2_reg_critic=.01,\n",
    "                 relu_alpha_actor=.01, relu_alpha_critic=.01,\n",
    "                 dropout_actor=0, dropout_critic=0,\n",
    "                 hidden_layer_sizes_actor=[32,64,32],\n",
    "                 hidden_layer_sizes_critic=[[32,64],[32,64]], ):\n",
    "\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        self.action_low = env.action_space.low\n",
    "        self.action_high = env.action_space.high\n",
    "\n",
    "        self.train_during_episode = train_during_episode\n",
    "\n",
    "        # Actor (Policy) Model\n",
    "        self.actor_local = Actor(self.state_size, self.action_size, self.action_low,\n",
    "                self.action_high, activation_fn=activation_fn_actor, relu_alpha=relu_alpha_actor,\n",
    "                bn_momentum=bn_momentum_actor, learn_rate=lr_actor, l2_reg=l2_reg_actor,\n",
    "                dropout=dropout_actor, hidden_layer_sizes=hidden_layer_sizes_actor, )\n",
    "        self.actor_target = Actor(self.state_size, self.action_size, self.action_low,\n",
    "                self.action_high, activation_fn=activation_fn_actor, relu_alpha=relu_alpha_actor,\n",
    "                bn_momentum=bn_momentum_actor, learn_rate=lr_actor, l2_reg=l2_reg_actor,\n",
    "                dropout=dropout_actor, hidden_layer_sizes=hidden_layer_sizes_actor, )\n",
    "\n",
    "        # Critic (Q-Value) Model\n",
    "        self.critic_local = Critic(self.state_size, self.action_size, l2_reg=l2_reg_critic,\n",
    "                learn_rate=lr_critic, bn_momentum=bn_momentum_critic, relu_alpha=relu_alpha_critic,\n",
    "                hidden_layer_sizes=hidden_layer_sizes_critic, dropout=dropout_critic, )\n",
    "        self.critic_target = Critic(self.state_size, self.action_size, l2_reg=l2_reg_critic,\n",
    "                learn_rate=lr_critic, bn_momentum=bn_momentum_critic, relu_alpha=relu_alpha_critic,\n",
    "                hidden_layer_sizes=hidden_layer_sizes_critic, dropout=dropout_critic, )\n",
    "\n",
    "        # Initialize target model parameters with local model parameters\n",
    "        self.critic_target.model.set_weights(self.critic_local.model.get_weights())\n",
    "        self.actor_target.model.set_weights(self.actor_local.model.get_weights())\n",
    "\n",
    "        # Noise process\n",
    "        self.exploration_mu = ou_mu\n",
    "        self.exploration_theta = ou_theta\n",
    "        self.exploration_sigma = ou_sigma\n",
    "        self.noise = OUNoise(self.action_size,\n",
    "                             self.exploration_mu, self.exploration_theta, self.exploration_sigma)\n",
    "\n",
    "        # Replay memory\n",
    "        self.buffer_size = replay_buffer_size\n",
    "        self.batch_size = replay_batch_size\n",
    "        self.memory = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "\n",
    "        # Algorithm parameters\n",
    "        self.gamma = discount_factor  # discount factor\n",
    "        self.lr_actor = lr_actor\n",
    "        self.lr_critic = lr_critic\n",
    "        self.dropout_actor = dropout_actor\n",
    "        self.dropout_critic = dropout_critic\n",
    "        self.bn_momentum_actor = bn_momentum_actor\n",
    "        self.bn_momentum_critic = bn_momentum_critic\n",
    "        self.activation_fn_actor = activation_fn_actor\n",
    "        self.ou_mu=ou_mu\n",
    "        self.ou_theta=ou_theta\n",
    "        self.ou_sigma=ou_sigma\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.replay_batch_size = replay_batch_size\n",
    "        self.l2_reg_actor = l2_reg_actor\n",
    "        self.l2_reg_critic = l2_reg_critic\n",
    "        self.relu_alpha_actor = relu_alpha_actor\n",
    "        self.relu_alpha_critic = relu_alpha_critic\n",
    "        self.hidden_layer_sizes_actor = hidden_layer_sizes_actor\n",
    "        self.hidden_layer_sizes_critic = hidden_layer_sizes_critic\n",
    "\n",
    "        self.tau_actor = tau_actor\n",
    "        self.tau_critic = tau_critic\n",
    "\n",
    "        # Training history\n",
    "        self.training_scores = []\n",
    "        self.test_scores = []\n",
    "        self.training_history = TrainingHistory(env)\n",
    "        self.q_a_frames_spec = Q_a_frames_spec(env)\n",
    "\n",
    "        # Track training steps and episodes\n",
    "        self.steps = 0\n",
    "        self.episodes = 0\n",
    "\n",
    "        self.reset_episode()\n",
    "\n",
    "    def print_summary(self):\n",
    "        print(\"Actor model summary:\")\n",
    "        self.actor_local.model.summary()\n",
    "        print(\"Critic model summary:\")\n",
    "        self.critic_local.model.summary()\n",
    "        print(\"Hyperparameters:\")\n",
    "        print(str(dict(\n",
    "            train_during_episode=self.train_during_episode,\n",
    "            discount_factor=self.gamma,\n",
    "            tau_actor=self.tau_actor, tau_critic=self.tau_critic,\n",
    "            lr_actor=self.lr_actor, lr_critic=self.lr_critic,\n",
    "            bn_momentum_actor=self.bn_momentum_actor,\n",
    "            bn_momentum_critic=self.bn_momentum_critic,\n",
    "            ou_mu=self.ou_mu, ou_theta=self.ou_theta, ou_sigma=1,\n",
    "            activation_fn_actor=self.activation_fn_actor,\n",
    "            replay_buffer_size=self.replay_buffer_size,\n",
    "            replay_batch_size=self.replay_batch_size,\n",
    "            l2_reg_actor=self.l2_reg_actor, l2_reg_critic=self.l2_reg_critic,\n",
    "            relu_alpha_actor=self.relu_alpha_actor,\n",
    "            relu_alpha_critic=self.relu_alpha_critic,\n",
    "            dropout_actor=self.dropout_actor, dropout_critic=self.dropout_critic,\n",
    "            hidden_layer_sizes_actor=self.hidden_layer_sizes_actor,\n",
    "            hidden_layer_sizes_critic=self.hidden_layer_sizes_critic, )))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        obs_space = self.env.observation_space\n",
    "        return np.array([\n",
    "            (state[i]-obs_space.low[i])/(obs_space.high[i]-obs_space.low[i])*2 - 1\n",
    "            for i in range(len(obs_space.low))])\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.noise.reset()\n",
    "        state = self.preprocess_state(self.env.reset())\n",
    "        self.last_state = state\n",
    "        return state\n",
    "\n",
    "    def step(self, action, reward, next_state, done):\n",
    "         # Save experience / reward\n",
    "        next_state = self.preprocess_state(next_state)\n",
    "        self.memory.add(self.last_state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > self.batch_size and (self.train_during_episode or done):\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "\n",
    "        # Roll over last state and action\n",
    "        self.last_state = next_state\n",
    "        self.steps += 1\n",
    "\n",
    "    def act(self, state=None, eps=0, verbose=False):\n",
    "        \"\"\"Returns actions for given state(s) as per current policy.\"\"\"\n",
    "        if state is None:\n",
    "            state = self.last_state\n",
    "        else:\n",
    "            state = self.preprocess_state(state)\n",
    "        state = np.reshape(state, [-1, self.state_size])\n",
    "        action = self.actor_local.model.predict(state)[0]\n",
    "        noise_sample = self.noise.sample() * max(0,eps)\n",
    "        res = list(np.clip(action + noise_sample, self.action_low, self.action_high))\n",
    "        if verbose:\n",
    "            print(\"State: (%6.3f, %6.3f), Eps: %6.3f, Action: %6.3f + %6.3f = %6.3f\"%\n",
    "                  (state[0][0], state[0][1], eps, action, noise_sample, res[0]))\n",
    "        return res  # add some noise for exploration\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\"\"\"\n",
    "        # Convert experience tuples to separate arrays for each element (states, actions, rewards, etc.)\n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        actions = np.array([e.action for e in experiences if e is not None]\n",
    "                          ).astype(np.float32).reshape(-1, self.action_size)\n",
    "        rewards = np.array([e.reward for e in experiences if e is not None]\n",
    "                          ).astype(np.float32).reshape(-1, 1)\n",
    "        dones = np.array([e.done for e in experiences if e is not None]\n",
    "                        ).astype(np.uint8).reshape(-1, 1)\n",
    "        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n",
    "\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target.model.predict_on_batch(next_states)\n",
    "        Q_targets_next = self.critic_target.model.predict_on_batch([next_states, actions_next])\n",
    "\n",
    "        # Compute Q targets for current states and train critic model (local)\n",
    "        Q_targets = rewards + self.gamma * Q_targets_next * (1 - dones)\n",
    "        self.critic_local.model.train_on_batch(x=[states, actions], y=Q_targets)\n",
    "\n",
    "        # Train actor model (local)\n",
    "        action_gradients = np.reshape(\n",
    "            self.critic_local.get_action_gradients([states, actions, 0]), (-1, self.action_size))\n",
    "        self.actor_local.train_fn([states, action_gradients, 1])  # custom training function\n",
    "\n",
    "        # Soft-update target models\n",
    "        self.soft_update(self.critic_local.model, self.critic_target.model, self.tau_critic)\n",
    "        self.soft_update(self.actor_local.model, self.actor_target.model, self.tau_actor)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\"\"\"\n",
    "        local_weights = np.array(local_model.get_weights())\n",
    "        target_weights = np.array(target_model.get_weights())\n",
    "\n",
    "        assert len(local_weights) == len(target_weights), \\\n",
    "            \"Local and target model parameters must have the same size\"\n",
    "\n",
    "        new_weights = tau * local_weights + (1 - tau) * target_weights\n",
    "        target_model.set_weights(new_weights)\n",
    "\n",
    "    def train_n_episodes(self, n_episodes, eps=1, eps_decay=None, action_repeat=1,\n",
    "                         run_tests=True, gen_q_a_frames_every_n_steps=0, draw_plots=False ):\n",
    "        if eps_decay is None: eps_decay = 1/n_episodes\n",
    "        n_training_episodes = len(self.training_scores)\n",
    "        for i_episode in range(n_training_episodes+1, n_training_episodes+n_episodes+1):\n",
    "            eps -= eps_decay\n",
    "            eps = max(eps,0)\n",
    "            episode_start_step = self.steps\n",
    "            self.run_episode(train=True, action_repeat=action_repeat, eps=eps,\n",
    "                             gen_q_a_frames_every_n_steps=gen_q_a_frames_every_n_steps )\n",
    "            if run_tests is True:\n",
    "                self.run_episode(train=False, eps=0, action_repeat=action_repeat)\n",
    "            message = \"Episode %i - epsilon: %.2f, memory size: %i, training score: %.2f\"\\\n",
    "                        %(self.episodes, eps, len(self.memory), self.training_history.training_episodes[-1].score)\n",
    "            if run_tests: message += \", test score: %.2f\"%self.training_history.test_episodes[-1].score\n",
    "            \n",
    "            lel.append(self.training_history.test_episodes[-1].score)\n",
    "            print(message)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    def run_episode(self, action_repeat=1, eps=0, train=False, gen_q_a_frames_every_n_steps=0 ):\n",
    "        next_state = self.reset_episode()\n",
    "        if train: episode_history = self.training_history.new_training_episode(self.episodes+1,eps)\n",
    "        else: episode_history = self.training_history.new_test_episode(self.episodes,eps)\n",
    "        q_a_frame = None\n",
    "        while True:\n",
    "            action = self.act(next_state, eps=eps)\n",
    "            sum_rewards=0\n",
    "            # Repeat action `action_repeat` times, summing up rewards\n",
    "            for i in range(action_repeat):\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                sum_rewards += reward\n",
    "                if done:\n",
    "                    break\n",
    "            #sum_rewards = np.log1p(sum_rewards)\n",
    "            episode_history.append(self.steps, next_state, action, sum_rewards)\n",
    "            if train:\n",
    "                self.step(action, sum_rewards, next_state, done)\n",
    "                if gen_q_a_frames_every_n_steps > 0 and self.steps%gen_q_a_frames_every_n_steps==0:\n",
    "                    self.training_history.add_q_a_frame(self.get_q_a_frames())\n",
    "            if done:\n",
    "                if train:\n",
    "                    self.episodes += 1\n",
    "                break\n",
    "\n",
    "    def get_q_a_frames(self):\n",
    "        \"\"\" TODO: Figure out how to work with added dimensions.\n",
    "                - Use x_dim, y_dim, and a_dim to know which dimensions of state and action to vary.\n",
    "                    Maybe fill in the unvaried dimensions of states and actions with agent's current state\n",
    "                    and anticipated action (according to policy).\n",
    "        \"\"\"\n",
    "        xs = self.q_a_frames_spec.xs\n",
    "        nx = self.q_a_frames_spec.nx\n",
    "        ys = self.q_a_frames_spec.ys\n",
    "        ny = self.q_a_frames_spec.ny\n",
    "        action_space = self.q_a_frames_spec.action_space\n",
    "        na = self.q_a_frames_spec.na\n",
    "        x_dim = self.q_a_frames_spec.x_dim\n",
    "        y_dim = self.q_a_frames_spec.y_dim\n",
    "        a_dim = self.q_a_frames_spec.a_dim\n",
    "\n",
    "        def get_state(x,y):\n",
    "            s=copy.copy(self.last_state)\n",
    "            s[x_dim]=x\n",
    "            s[y_dim]=y\n",
    "            return s\n",
    "        raw_states = np.array([[ get_state(x,y) for x in xs ] for y in ys ]).reshape(nx*ny, self.state_size)\n",
    "\n",
    "        def get_action(action):\n",
    "            a=self.act() if self.action_size>1 else [0]\n",
    "            a[a_dim]=action\n",
    "            return a\n",
    "        actions = np.array([get_action(a) for a in action_space]*nx*ny)\n",
    "\n",
    "        preprocessed_states = np.array([ self.preprocess_state(s) for s in raw_states])\n",
    "        Q = self.critic_local.model.predict_on_batch(\n",
    "            [np.repeat(preprocessed_states,na,axis=0),actions]).reshape((ny,nx,na))\n",
    "        Q_max = np.max(Q,axis=2)\n",
    "        Q_std = np.std(Q,axis=2)\n",
    "        max_action = np.array([action_space[a] for a in np.argmax(Q,axis=2).flatten()]).reshape((ny,nx))\n",
    "        actor_policy = np.array([ self.act(s)[0] for s in raw_states]).reshape(ny,nx)\n",
    "        action_gradients = self.critic_local.get_action_gradients(\n",
    "            [preprocessed_states,actor_policy.reshape(nx*ny,-1),0])[0].reshape(ny,nx)\n",
    "\n",
    "        return namedtuple( 'q_a_frames',[\n",
    "                'step_idx', 'episode_idx', 'Q_max', 'Q_std', 'max_action', 'action_gradients', 'actor_policy'\n",
    "            ])(self.steps, self.episodes, Q_max, Q_std, max_action, action_gradients, actor_policy)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size: maximum size of buffer\n",
    "            batch_size: size of each training batch\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\",\n",
    "                                     field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        return random.sample(self.memory, k=min(self.batch_size, len(self)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck noise process.\"\"\"\n",
    "    def __init__(self, size, mu, theta, sigma):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self, sigma=None):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        if sigma is None:\n",
    "            sigma = self.sigma\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "class Actor:\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, action_low, action_high, learn_rate,\n",
    "                 activation_fn, bn_momentum, relu_alpha, l2_reg, dropout, hidden_layer_sizes):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state.\n",
    "            action_size (int): Dimension of each action.\n",
    "            action_low (array): Min value of each action dimension.\n",
    "            action_high (array): Max value of each action dimension.\n",
    "            learn_rate (float): Learning rate.\n",
    "            activation_fn (string): Activation function, either 'sigmoid' or 'tanh'.\n",
    "            bn_momentum (float): Batch Normalization momentum .\n",
    "            relu_alpha (float): LeakyReLU alpha, allowing small gradient when the unit is not active.\n",
    "            l2_reg (float): L2 regularization factor for each dense layer.\n",
    "            dropout (float): Dropout rate\n",
    "            hidden_layer_sizes (list): List of hidden layer sizes.\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.action_low = action_low\n",
    "        self.action_high = action_high\n",
    "        self.action_range = self.action_high - self.action_low\n",
    "\n",
    "        # Initialize any other variables here\n",
    "        self.learn_rate = learn_rate\n",
    "        self.activation = activation_fn\n",
    "        self.bn_momentum = bn_momentum\n",
    "        self.relu_alpha = relu_alpha\n",
    "        self.l2_reg = l2_reg\n",
    "        self.dropout = dropout\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        states = layers.Input(shape=(self.state_size,), name='states')\n",
    "        net = states\n",
    "\n",
    "        # Batch Norm instead of input preprocessing (Since we don't know up front the range of state values.)\n",
    "        #net = layers.BatchNormalization(momentum=self.bn_momentum)(states)\n",
    "\n",
    "        # Add a hidden layer for each element of hidden_layer_sizes\n",
    "        for size in self.hidden_layer_sizes:\n",
    "            net = layers.Dense(units=size, kernel_regularizer=regularizers.l2(l=self.l2_reg))(net)\n",
    "            #net = layers.BatchNormalization(momentum=self.bn_momentum)(net)\n",
    "            if self.relu_alpha>0: net = layers.LeakyReLU(alpha=self.relu_alpha)(net)\n",
    "            else: net = layers.Activation('relu')(net)\n",
    "\n",
    "        if self.dropout>0: net = layers.Dropout(.2)(net)\n",
    "\n",
    "        if self.bn_momentum>0: net = layers.BatchNormalization(momentum=self.bn_momentum)(net)\n",
    "\n",
    "        if self.activation=='tanh':\n",
    "            # Add final output layer with tanh activation with [-1, 1] output\n",
    "            actions = layers.Dense(units=self.action_size, activation=self.activation,\n",
    "                name='actions')(net)\n",
    "        elif self.activation=='sigmoid':\n",
    "            # Add final output layer with sigmoid activation\n",
    "            raw_actions = layers.Dense(units=self.action_size, activation=self.activation,\n",
    "                name='raw_actions')(net)\n",
    "            # Scale [0, 1] output for each action dimension to proper range\n",
    "            actions = layers.Lambda(lambda x: (x * self.action_range) + self.action_low,\n",
    "                name='actions')(raw_actions)\n",
    "        else:\n",
    "            raise \"Expected 'activation' to be one of: 'tanh', or 'sigmoid'.\"\n",
    "\n",
    "        self.model = models.Model(inputs=states, outputs=actions)\n",
    "        action_gradients = layers.Input(shape=(self.action_size,))\n",
    "        loss = K.mean(-action_gradients * actions)\n",
    "\n",
    "        # Incorporate any additional losses here (e.g. from regularizers)\n",
    "\n",
    "        optimizer = optimizers.Adam(lr=self.learn_rate)\n",
    "        updates_op = optimizer.get_updates(params=self.model.trainable_weights, loss=loss)\n",
    "        self.train_fn = K.function(\n",
    "            inputs=[self.model.input, action_gradients, K.learning_phase()],\n",
    "            outputs=[],\n",
    "            updates=updates_op)\n",
    "\n",
    "class Critic:\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, learn_rate, bn_momentum,\n",
    "                 relu_alpha, l2_reg, dropout, hidden_layer_sizes,\n",
    "                ):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state.\n",
    "            action_size (int): Dimension of each action.\n",
    "            learn_rate (float): Learning rate.\n",
    "            bn_momentum (float): Batch Normalization momentum.\n",
    "            relu_alpha (float): LeakyReLU alpha, allowing small gradient when the unit is not active.\n",
    "            l2_reg (float): L2 regularization factor for each dense layer.\n",
    "            dropout (float): Dropout rate\n",
    "            hidden_layer_sizes (list[list]): List of two lists with hidden layer sizes for state and action pathways.\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        assert len(hidden_layer_sizes)==2 \\\n",
    "            and len(hidden_layer_sizes[0])==len(hidden_layer_sizes[1]),\\\n",
    "            \"Expected Critic's hidden_layer_sizes to be a list of two arrays of equal length.\"\n",
    "\n",
    "        # Initialize any other variables here\n",
    "        self.learn_rate = learn_rate\n",
    "        self.bn_momentum = bn_momentum\n",
    "        self.relu_alpha = relu_alpha\n",
    "        self.l2_reg = l2_reg\n",
    "        self.dropout = dropout\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        states = layers.Input(shape=(self.state_size,), name='states')\n",
    "        actions = layers.Input(shape=(self.action_size,), name='actions')\n",
    "        net_states = states\n",
    "        net_actions = actions\n",
    "\n",
    "        # Add hidden layer(s) for state pathway\n",
    "        for size in self.hidden_layer_sizes[0]:\n",
    "            net_states = layers.Dense(units=size,\n",
    "                                      kernel_regularizer=regularizers.l2(l=self.l2_reg))(net_states)\n",
    "            #net_states = layers.BatchNormalization(momentum=self.bn_momentum)(net_states)\n",
    "            if self.relu_alpha>0: net_states = layers.LeakyReLU(alpha=self.relu_alpha)(net_states)\n",
    "            else: net_states = layers.Activation('relu')(net_states)\n",
    "\n",
    "        # Add hidden layer(s) for action pathway\n",
    "        for size in self.hidden_layer_sizes[1]:\n",
    "            net_actions = layers.Dense(units=size,\n",
    "                                       kernel_regularizer=regularizers.l2(l=self.l2_reg))(net_actions)\n",
    "            #net_actions = layers.BatchNormalization(momentum=self.bn_momentum)(net_actions)\n",
    "            if self.relu_alpha>0: net_actions = layers.LeakyReLU(alpha=self.relu_alpha)(net_actions)\n",
    "            else: net_actions = layers.Activation('relu')(net_actions)\n",
    "\n",
    "        # Combine state and action pathways\n",
    "        net = layers.Add()([net_states, net_actions])\n",
    "        if self.relu_alpha>0: net = layers.LeakyReLU(alpha=self.relu_alpha)(net)\n",
    "        else: net = layers.Activation('relu')(net)\n",
    "\n",
    "        # Add more layers to the combined network if needed\n",
    "        if self.dropout>0: net = layers.Dropout(self.dropout)(net)\n",
    "\n",
    "        # Normalize the final activations\n",
    "        if self.bn_momentum>0: net = layers.BatchNormalization(momentum=self.bn_momentum)(net)\n",
    "\n",
    "        # Add final output layer to prduce action values (Q values)\n",
    "        Q_values = layers.Dense(units=1, name='q_values')(net)\n",
    "\n",
    "        self.model = models.Model(inputs=[states, actions], outputs=Q_values)\n",
    "\n",
    "        # Define optimizer and compile model for training with built-in loss function\n",
    "        optimizer = optimizers.Adam(lr=self.learn_rate)\n",
    "        self.model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        # Compute action gradients (derivative of Q values w.r.t. to actions)\n",
    "        action_gradients = K.gradients(Q_values, actions)\n",
    "\n",
    "        # Define an additional function to fetch action gradients (to be used by actor model)\n",
    "        self.get_action_gradients = K.function(\n",
    "            inputs=[*self.model.input, K.learning_phase()],\n",
    "            outputs=action_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class TrainingHistory:\n",
    "    \"\"\"\n",
    "    Tracks training history, including a snapshot of rasterized Q values and actions\n",
    "    for use in visualizations.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, nx=16, ny=16, na=11, x_dim=0, y_dim=1, a_dim=0):\n",
    "        \"\"\"\n",
    "        Initialize TrainingHistory object with Q grid shape.\n",
    "        Params\n",
    "        ======\n",
    "        \"\"\"\n",
    "        self.training_episodes = []\n",
    "        self.test_episodes = []\n",
    "        self.q_a_frames = []\n",
    "        self.last_step = 0\n",
    "        self.q_a_frames_spec = Q_a_frames_spec(env, nx=nx, ny=ny, na=na,\n",
    "                                         x_dim=x_dim, y_dim=y_dim, a_dim=a_dim)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"TrainingHistory ( %i training_episodes, %i test_episodes, %i qa_grids, last_step: %i )\"%\\\n",
    "                (len(self.training_episodes), len(self.test_episodes), len(self.q_a_frames), self.last_step)\n",
    "\n",
    "    def add_q_a_frame(self, q_a_frame):\n",
    "        self.q_a_frames.append(q_a_frame)\n",
    "\n",
    "    def new_training_episode(self, idx, epsilon=None):\n",
    "        episode = EpisodeHistory(idx, epsilon)\n",
    "        self.training_episodes.append(episode)\n",
    "        return episode\n",
    "\n",
    "    def new_test_episode(self, idx, epsilon=None):\n",
    "        episode = EpisodeHistory(idx, epsilon)\n",
    "        self.test_episodes.append(episode)\n",
    "        return episode\n",
    "\n",
    "    def get_training_episode_for_step(self, step_idx):\n",
    "        for ep in self.training_episodes:\n",
    "            if ep.last_step>=step_idx:\n",
    "                return ep\n",
    "    def get_test_episode_for_step(self, step_idx):\n",
    "        for ep in self.test_episodes:\n",
    "            if (ep.last_step+1)>=step_idx:\n",
    "                return ep\n",
    "    def get_q_a_frames_for_step(self, step_idx):\n",
    "        for g in self.q_a_frames:\n",
    "            if g.step_idx>=step_idx:\n",
    "                return g\n",
    "        return g\n",
    "    def append_training_step(self, step, state, action, reward):\n",
    "        \"\"\"\n",
    "        Initialize EpisodeHistory with states, actions, and rewards\n",
    "        Params\n",
    "        ======\n",
    "            episode_idx (int): Episode index\n",
    "            step (int): Step index\n",
    "            state (list|array): State, array-like of shape env.observation_space.shape\n",
    "            action (list|array): Action, array-like of shape env.action_space.shape\n",
    "            reward (float): Reward, scalar value\n",
    "        \"\"\"\n",
    "        if len(self.training_episodes)==0:\n",
    "            raise \"No training episodes exist yet. \"\n",
    "        self.training_episodes[-1].append(step, state, action, reward)\n",
    "        self.last_step = step\n",
    "        return self\n",
    "    def append_test_step(self, step, state, action, reward):\n",
    "        \"\"\"\n",
    "        Initialize EpisodeHistory with states, actions, and rewards\n",
    "        Params\n",
    "        ======\n",
    "            episode_idx (int): Episode index\n",
    "            step (int): Step index\n",
    "            state (list|array): State, array-like of shape env.observation_space.shape\n",
    "            action (list|array): Action, array-like of shape env.action_space.shape\n",
    "            reward (float): Reward, scalar value\n",
    "        \"\"\"\n",
    "        if len(self.test_episodes)==0:\n",
    "            raise \"No test episodes exist yet. \"\n",
    "        self.test_episodes[-1].append(step, state, action, reward)\n",
    "        self.last_step = step\n",
    "        return self\n",
    "    \n",
    "class EpisodeHistory:\n",
    "    \"\"\" Tracks the history for a single episode, including the states, actions, and rewards.\n",
    "    \"\"\"\n",
    "    def __init__(self, episode_idx=None, epsilon=None):\n",
    "        \"\"\"\n",
    "        Initialize EpisodeHistory with states, actions, and rewards\n",
    "        Params\n",
    "        ======\n",
    "            episode_idx (int): Episode index\n",
    "            epsilon (float): Exploration factor\n",
    "        \"\"\"\n",
    "        self.episode_idx = episode_idx\n",
    "        self.epsilon = epsilon\n",
    "        self.steps = []\n",
    "        self.first_step = None\n",
    "        self.last_step = None\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "        self.score = self._get_score()\n",
    "\n",
    "    def _get_score(self):\n",
    "        return sum(self.rewards)\n",
    "\n",
    "    def append(self, step, state, action, reward):\n",
    "        self.steps.append(step)\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        if self.first_step is None: self.first_step = step\n",
    "        self.last_step = step\n",
    "        self.score += reward\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"EpisodeHistory ( idx: %i, len: %i, first_step: %i, last_step: %i, epsilon: %.3f, score: %.3f )\"%\\\n",
    "            (self.episode_idx,len(self.steps),self.first_step, self.last_step, self.epsilon, self.score)\n",
    "    \n",
    "class Q_a_frames_spec():\n",
    "    \"\"\" \n",
    "    Tracks training history, including a snapshot of rasterized Q values and actions\n",
    "    for use in visualizations. \n",
    "    \"\"\"\n",
    "    def __init__(self, env, nx=16, ny=16, na=11, x_dim=0, y_dim=1, a_dim=0):\n",
    "        \"\"\"\n",
    "        Initialize Q_a_frame_set object with Q grid shape.\n",
    "        Params\n",
    "        ======            \n",
    "             env (obj): OpenAi Gym environment\n",
    "             nx (int): Width of Q grid (default: 16)\n",
    "             ny (int): Height of Q grid (default: 16)\n",
    "             na (int): Depth of Q grid (default: 11)\n",
    "             x_dim (int): Observation dimension to use as x-axis (default: 0)\n",
    "             y_dim (int): Observation dimension to use as y-axis (default: 1)\n",
    "             a_dim (int): Action dimension to use as x-axis (default: 0)\n",
    "        \"\"\"\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.a_dim = a_dim\n",
    "        \n",
    "        self.xmin = env.observation_space.low[x_dim]\n",
    "        self.xmax = env.observation_space.high[x_dim]\n",
    "        self.xs = np.arange(self.xmin, self.xmax, (self.xmax-self.xmin)/nx)[:nx]\n",
    "        self.nx = len(self.xs)\n",
    "        \n",
    "        self.ymin = env.observation_space.low[y_dim]\n",
    "        self.ymax = env.observation_space.high[y_dim]\n",
    "        self.ys = np.arange(self.ymin, self.ymax, (self.ymax-self.ymin)/ny)[:ny]\n",
    "        self.ny = len(self.ys)\n",
    "        \n",
    "        self.amin = env.action_space.low[a_dim]\n",
    "        self.amax = env.action_space.high[a_dim]\n",
    "        self.action_space = np.linspace(self.amin,self.amax,na)\n",
    "        self.na = len(self.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor model summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "states (InputLayer)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "actions (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,137\n",
      "Trainable params: 1,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Critic model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "states (InputLayer)             (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "actions (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           48          states[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           32          actions[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 16)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 16)           0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 32)           544         leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 32)           544         leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 32)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 32)           0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32)           0           leaky_re_lu_8[0][0]              \n",
      "                                                                 leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 32)           0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32)           128         leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "q_values (Dense)                (None, 1)            33          batch_normalization_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,329\n",
      "Trainable params: 1,265\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n",
      "Hyperparameters:\n",
      "{'train_during_episode': True, 'discount_factor': 0.999, 'tau_actor': 0.3, 'tau_critic': 0.1, 'lr_actor': 0.0001, 'lr_critic': 0.005, 'bn_momentum_actor': 0, 'bn_momentum_critic': 0.7, 'ou_mu': 0, 'ou_theta': 0.05, 'ou_sigma': 1, 'activation_fn_actor': 'tanh', 'replay_buffer_size': 10000, 'replay_batch_size': 1024, 'l2_reg_actor': 0.01, 'l2_reg_critic': 0.01, 'relu_alpha_actor': 0.01, 'relu_alpha_critic': 0.01, 'dropout_actor': 0, 'dropout_critic': 0, 'hidden_layer_sizes_actor': [16, 32, 16], 'hidden_layer_sizes_critic': [[16, 32], [16, 32]]}\n"
     ]
    }
   ],
   "source": [
    "agent = DDPG(env, train_during_episode=True, ou_mu=0, ou_theta=.05, ou_sigma=.25, \n",
    "             discount_factor=.999, replay_buffer_size=10000, replay_batch_size=1024,\n",
    "             tau_actor=.3, tau_critic=.1, \n",
    "             relu_alpha_actor=.01, relu_alpha_critic=.01,\n",
    "             lr_actor=.0001, lr_critic=.005, activation_fn_actor='tanh',\n",
    "             l2_reg_actor=.01, l2_reg_critic=.001, \n",
    "             bn_momentum_actor=0, bn_momentum_critic=.7,\n",
    "             hidden_layer_sizes_actor=[16,32,16], hidden_layer_sizes_critic=[[16,32],[16,32]], )\n",
    "agent.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 - epsilon: 0.98, memory size: 200, training score: -40.04, test score: -1.09\n",
      "Episode 2 - epsilon: 0.96, memory size: 328, training score: 74.39, test score: -1.09\n",
      "Episode 3 - epsilon: 0.94, memory size: 396, training score: 80.42, test score: -1.12\n",
      "Episode 4 - epsilon: 0.92, memory size: 596, training score: -28.47, test score: -1.08\n",
      "Episode 5 - epsilon: 0.90, memory size: 687, training score: 74.34, test score: -1.14\n",
      "Episode 6 - epsilon: 0.88, memory size: 760, training score: 86.48, test score: -1.10\n",
      "Episode 7 - epsilon: 0.86, memory size: 779, training score: 93.94, test score: -1.09\n",
      "Episode 8 - epsilon: 0.84, memory size: 958, training score: 64.63, test score: -1.09\n",
      "Episode 9 - epsilon: 0.82, memory size: 1023, training score: 89.90, test score: -1.09\n",
      "Episode 10 - epsilon: 0.80, memory size: 1095, training score: 88.89, test score: -0.06\n",
      "Episode 11 - epsilon: 0.78, memory size: 1128, training score: 94.65, test score: -0.64\n",
      "Episode 12 - epsilon: 0.76, memory size: 1176, training score: 92.61, test score: -3.30\n",
      "Episode 13 - epsilon: 0.74, memory size: 1218, training score: 91.09, test score: -7.13\n",
      "Episode 14 - epsilon: 0.72, memory size: 1293, training score: 89.61, test score: -11.65\n",
      "Episode 15 - epsilon: 0.70, memory size: 1379, training score: 92.10, test score: 90.94\n",
      "Episode 16 - epsilon: 0.68, memory size: 1440, training score: 93.10, test score: 93.41\n",
      "Episode 17 - epsilon: 0.66, memory size: 1466, training score: 95.10, test score: 96.12\n",
      "Episode 18 - epsilon: 0.64, memory size: 1497, training score: 95.25, test score: 94.91\n",
      "Episode 19 - epsilon: 0.62, memory size: 1551, training score: 96.72, test score: 95.97\n",
      "Episode 20 - epsilon: 0.60, memory size: 1593, training score: 92.55, test score: 95.77\n",
      "Episode 21 - epsilon: 0.58, memory size: 1630, training score: 92.99, test score: 95.30\n",
      "Episode 22 - epsilon: 0.56, memory size: 1689, training score: 92.23, test score: 94.16\n",
      "Episode 23 - epsilon: 0.54, memory size: 1719, training score: 95.55, test score: 93.91\n",
      "Episode 24 - epsilon: 0.52, memory size: 1761, training score: 90.54, test score: 93.38\n",
      "Episode 25 - epsilon: 0.50, memory size: 1787, training score: 94.45, test score: 94.06\n",
      "Episode 26 - epsilon: 0.48, memory size: 1812, training score: 94.67, test score: 93.95\n",
      "Episode 27 - epsilon: 0.46, memory size: 1841, training score: 93.79, test score: 93.93\n",
      "Episode 28 - epsilon: 0.44, memory size: 1869, training score: 94.87, test score: 95.12\n",
      "Episode 29 - epsilon: 0.42, memory size: 1908, training score: 93.97, test score: 95.31\n",
      "Episode 30 - epsilon: 0.40, memory size: 1944, training score: 93.02, test score: 92.92\n",
      "Episode 31 - epsilon: 0.38, memory size: 1968, training score: 95.09, test score: 94.11\n",
      "Episode 32 - epsilon: 0.36, memory size: 2003, training score: 95.07, test score: 92.69\n",
      "Episode 33 - epsilon: 0.34, memory size: 2055, training score: 92.10, test score: 94.61\n",
      "Episode 34 - epsilon: 0.32, memory size: 2078, training score: 95.57, test score: 93.03\n",
      "Episode 35 - epsilon: 0.30, memory size: 2108, training score: 91.06, test score: 95.71\n",
      "Episode 36 - epsilon: 0.28, memory size: 2150, training score: 93.08, test score: 93.77\n",
      "Episode 37 - epsilon: 0.26, memory size: 2174, training score: 95.06, test score: 95.49\n",
      "Episode 38 - epsilon: 0.24, memory size: 2198, training score: 95.29, test score: 94.48\n",
      "Episode 39 - epsilon: 0.22, memory size: 2224, training score: 95.31, test score: 95.41\n",
      "Episode 40 - epsilon: 0.20, memory size: 2262, training score: 94.80, test score: 93.85\n",
      "Episode 41 - epsilon: 0.18, memory size: 2286, training score: 94.97, test score: 95.22\n",
      "Episode 42 - epsilon: 0.16, memory size: 2311, training score: 96.09, test score: 92.68\n",
      "Episode 43 - epsilon: 0.14, memory size: 2336, training score: 94.98, test score: 95.24\n",
      "Episode 44 - epsilon: 0.12, memory size: 2367, training score: 94.96, test score: 94.95\n",
      "Episode 45 - epsilon: 0.10, memory size: 2392, training score: 94.82, test score: 94.69\n",
      "Episode 46 - epsilon: 0.08, memory size: 2416, training score: 95.26, test score: 94.38\n",
      "Episode 47 - epsilon: 0.06, memory size: 2444, training score: 95.02, test score: 94.88\n",
      "Episode 48 - epsilon: 0.04, memory size: 2475, training score: 94.90, test score: 94.37\n",
      "Episode 49 - epsilon: 0.02, memory size: 2506, training score: 94.89, test score: 94.97\n",
      "Episode 50 - epsilon: 0.00, memory size: 2537, training score: 95.17, test score: 95.29\n"
     ]
    }
   ],
   "source": [
    "agent.train_n_episodes(50, eps=3, eps_decay=1/50, action_repeat=5, \n",
    "                       run_tests=True, gen_q_a_frames_every_n_steps=10, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([i for i in range (50)],lel)\n",
    "plt.title(\"MountainCarContinuous\")\n",
    "plt.xlabel('episode number')\n",
    "plt.ylabel('rewards')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
